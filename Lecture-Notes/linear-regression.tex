\chapter{Linear Regression}
A great deal of the current success of artificial intelligence is due to recent advances in
\href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning}.  
In order to get a first taste of what machine learning is about, we introduce 
\href{https://en.wikipedia.org/wiki/Linear_regression}{linear regression} in this chapter, since linear regression
is one of the most basic algorithms in machine learning.  It is also the foundation for more advanced
forms of machine learning like \href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression} and 
\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}.
Furthermore, linear regression is surprisingly powerful.  Finally, many of the fundamental problems of machine
learning can already be illustrated with linear regression.  Therefore it is only natural that we begin our
study of machine learning with the study of linear regression.

\section{Simple Linear Regression}
Assume we want to know how the \href{https://en.wikipedia.org/wiki/Engine_displacement}{engine displacement} of
a car engine relates to the fuel consumption of the car.  Of course, we could try to create a theoretical model that
relates the engine displacement to its fuel consumption.  However, due to our lack of understanding of engine
physics, this is not an option for us.  Instead, we could try to take a look at different engines and compare
their engine displacement with the corresponding fuel consumption.  This way, we would collect a set of  
$m$ observations of the form $\langle x_1, y_1\rangle, \cdots, \langle x_m, y_m\rangle$ 
where $x_i$ is the engine displacement of the engine in the $i$-th car, while $y_i$ is the fuel consumption of the
$i$-th car.  We call $x$ the \blue{independent variable}, while $y$ is the 
\blue{dependent variable}.  We define the vectors $\mathbf{x}$ and $\mathbf{y}$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x} := \langle x_1, \cdots, x_m \rangle^\top$ \quad and \quad
$\mathbf{y} := \langle y_1, \cdots, y_m \rangle^\top$.
\\[0.2cm]
Here, the operator $^\top$ is interpreted as the \href{https://en.wikipedia.org/wiki/Transpose}{transpose} operator,
i.e.~$\mathbf{x}$ and $\mathbf{y}$ are considered to be column vectors.

In linear regression, we use a \blue{linear hypothesis} 
and assume that the dependent variable $y_i$ is related to the independent variable $x_i$ via a linear
equation of the form
\\[0.2cm]
\hspace*{1.3cm}
$y_i := \vartheta_1 \cdot x_i + \vartheta_0$.
\\[0.2cm]
We do not expect this equation to hold exactly.  The reason is that there are many other factors besides the
engine displacement that influence the fuel consumption.  For example, both the weight of a car and its 
\href{https://en.wikipedia.org/wiki/Automotive_aerodynamics}{aerodynamics} certainly influence the fuel consumption.  
We want to calculate those values $\vartheta_0$ and $\vartheta_1$ such that the 
\blue{\underline{m}ean \underline{s}quared \underline{e}rror}, which is defined as 
\begin{equation}
  \label{eq:mse}
 \mathtt{MSE}(\vartheta_0, \vartheta_1) := \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2,
\end{equation}
is minimized.  It can be shown that the solution to this minimization problem is given as follows:
\begin{equation}
  \label{eq:theta0}
   \vartheta_1 = r_{x,y} \cdot \frac{s_y}{s_x} \quad \mbox{and} \quad \ds\vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}.
\end{equation}
This solution makes use of the values $r_{x,y}$, $s_x$, and $s_y$.  In order to define these values, we first
define the \blue{sample mean values} $\bar{\mathbf{x}}$ and $\bar{\mathbf{y}}$ of $\mathbf{x}$ and $\mathbf{y}$ respectively, i.e.~we have 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \bar{\mathbf{x}} = \frac{1}{m} \cdot \sum\limits_{i=1}^m x_i$ \quad and \quad
$\ds \bar{\mathbf{y}} = \frac{1}{m} \cdot \sum\limits_{i=1}^m y_i$.
\\[0.2cm]
Furthermore, $s_x$ and $s_y$ are the \blue{sample standard deviations} of $\mathbf{x}$ and $y$, i.e.~we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds s_x = \sqrt{\frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2\;}$ \quad and \quad
$\ds s_y = \sqrt{\frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2\;}$.
\\[0.2cm]
Next, $\mathrm{Cov}[\mathbf{x}, \mathbf{y}]$ is the \blue{sample covariance} and is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathrm{Cov}[\mathbf{x}, \mathbf{y}] = \frac{1}{(m-1)} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)$.
\\[0.2cm]
Finally, $r_{x,y}$ is the \blue{sample correlation coefficient} that is defined as
\\[0.2cm]
\hspace*{1.3cm}
$\ds r_{x,y} = \frac{1}{(m-1) \cdot s_x \cdot s_y} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)
            = \frac{\mathrm{Cov}[\mathbf{x}, \mathbf{y}]}{s_x \cdot s_y}
$.
\\[0.2cm]
The number $r_{x,y}$ is also known as the
\href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Pearson correlation coefficient} or
\blue{Pearson's \textrm{r}}.  It is named after \href{https://en.wikipedia.org/wiki/Karl_Pearson}{Karl Pearson}
(1857 -- 1936).
Note that the formula for the parameter $\vartheta_1$ can be simplified to  
\begin{equation}
  \label{eq:theta1}
\ds\vartheta_1 = \frac{\sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)}{
                        \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2}  
\end{equation}
This latter formula should be used to calculate $\vartheta_1$.  However, the previous formula is also useful
because it shows the the correlation coefficient is identical to the coefficient $\vartheta_1$, provided the variables $\mathbf{x}$ and
$\mathbf{y}$ have been \blue{normalized} so that their standard deviation is $1$.

\exercise
Prove Equation \ref{eq:theta0} and Equation \ref{eq:theta1}.

\hint
Take the partial derivatives of $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ with respect to $\vartheta_0$ and
$\vartheta_1$.  In order to minimize the expression  $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ with respect to
$\vartheta_0$ and $\vartheta_1$, these derivatives have to be set to $0$.
\eox

\subsection{Assessing the Quality of Linear Regression}
Assume that we have been given a set of $m$ observations of the form $\langle x_1, y_1\rangle, \cdots, \langle x_m, y_m\rangle$  
and that we have calculated the parameters $\vartheta_0$ and $\vartheta_1$ according to Equation
\ref{eq:theta0} and Equation \ref{eq:theta1}.  Provided that not all $x_i$ have the same value, these
formul\ae\ will return two numbers for $\vartheta_0$ and $\vartheta_1$ that define a linear model for
$\mathbf{y}$ in terms of $\mathbf{x}$.  However, at the moment we still lack a number that tell us how good
this linear model really is.  In order to judge the quality of the linear model given by 
\\[0.2cm]
\hspace*{1.3cm}
$y = \vartheta_0 + \vartheta_1 \cdot x$
\\[0.2cm]
we can compute the mean squared error according to Equation \ref{eq:mse}.  However, the mean squared error 
is an absolute number that, by itself, is difficult to interpret.  The reason is that the variable $\mathbf{y}$ might be
inherently noisy and we have to relate this noise to the mean squared error.  Now the noise contained in $\mathbf{y}$
can be measured by the \blue{sample variance} of $\mathbf{y}$ and is given by the formula
\begin{equation}
  \label{eq:var}
  \mathtt{Var}(y) := \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(y_i - \bar{y}\bigr)^2.
\end{equation}
If we compare this formula to the formula for the mean squared error
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{MSE}(\vartheta_0, \vartheta_1) := 
  \frac{1}{m-1} \cdot\sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
$,
\\[0.2cm]
we see that the sample variance of $\mathbf{y}$ is an upper bound for the mean squared error since we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{Var}(\mathbf{y}) = \mathtt{MSE}(\bar{\mathbf{y}}, 0)$,
\\[0.2cm]
i.e.~the sample variance is the value that we would get for the mean squared error if we set $\vartheta_0$ to
the average value of $\mathbf{y}$ and $\vartheta_1$ to zero.  Since $\vartheta_0$ and $\vartheta_1$ are chosen to
minimize the mean squared error, we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{MSE}(\vartheta_0, \vartheta_1) \leq \mathtt{MSE}(\bar{\mathbf{y}}, 0) = \mathtt{Var}(\mathbf{y})$.
\\[0.2cm]
The mean squared error is an absolute value and, therefore, difficult to interpret.  The fraction
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\mathtt{MSE}(\vartheta_0, \vartheta_1)}{\mathtt{Var}(y)}$
\\[0.2cm]
is called \blue{the proportion of the unexplained variance} because it is the variance that is still
left if we use our linear model to predict the values of $\mathbf{y}$ given the values of $\mathbf{x}$.  The
\blue{proportion of the explained variance} which is also known as the \blue{$\mathtt{R}^2$ statistic} is defined as 
\begin{equation}
  \label{eq:Rsquare}
  \mathtt{R}^2 := \frac{\mathtt{Var}(\mathbf{y}) - \mathtt{MSE}}{\mathtt{Var}(\mathbf{y})} = 1 - \frac{\mathtt{MSE}}{\mathtt{Var}(\mathbf{y})}.
\end{equation}
Here $\mathtt{MSE}$ is short for $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ where we have substituted the values
from equation \ref{eq:theta0} and \ref{eq:theta1}.  The $\mathtt{R}^2$ statistic measures the quality of our
model: If it is small, then our model does not explain the variation of the value of $\mathbf{y}$ when the value of $\mathbf{x}$
changes.  On the other hand, if it is near to $100\%$, then our model does a good job in explaining the 
variation of $\mathbf{y}$ when $\mathbf{x}$ changes.

Since the formul\ae\ for $\mathtt{Var}(\mathbf{y})$ and $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ have the same
denominator $m-1$, this denominator can be cancelled when the $\mathtt{R}^2$ statistic is computed.  To this
end we define the \blue{total sum of squares} $\mathtt{TSS}$ as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{TSS} := \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2 = (m-1) \cdot \mathtt{Var}(\mathbf{y})$
\\[0.2cm]
and the \blue{residual sum of squares} $\mathtt{RSS}$ as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{RSS} := \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
                  = (m-1) \cdot \mathtt{MSE}(\vartheta_0, \vartheta_1)
$.
\\[0.2cm]
Then the formula for the $\mathtt{R}^2$ statistic can be written as
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathtt{R}^2 = 1 - \frac{\mathtt{RSS}}{\mathtt{TSS}}$.
\\[0.2cm]
This is the formula that we will use when we implement simple linear regression.

It should be noted that $\mathrm{R}^2$ is the square of Pearson's \textrm{r}.  The notation is a bit
inconsistent since Pearson's $r$ is written in lower case, while $\mathrm{R}^2$ is written in upper
case.  Finally $\mathrm{R}^2$ is also known as the 
\href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{coefficient of determination}.  It tells us
to what is extend the value of the variable $y$ is determined by the value of $x$.


\subsection{Putting the Theory to the Test}
In order to get a better feeling for linear regression, we want to test it to investigate the factors that
determine the fuel consumption of cars.  \myFig{cars.csv} shows the head of the data file 
``\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/SetlX/cars.csv}{\texttt{cars.csv}}''
which I have adapted from the file
\\[0.2cm]
\hspace*{1.3cm}
\href{http://www-bcf.usc.edu/~gareth/ISL/Auto.csv}{\texttt{http://www-bcf.usc.edu/\symbol{126}gareth/ISL/Auto.csv}}.
\\[0.2cm]
\myFig{cars.csv} shows the column headers and the first ten data entries contained in this file.  
Altogether, this file contains data of 392 different car models.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
     mpg, cyl, displacement,    hp, weight,  acc, year, name
    18.0,   8,        307.0, 130.0, 3504.0, 12.0,   70, chevrolet chevelle malibu
    15.0,   8,        350.0, 165.0, 3693.0, 11.5,   70, buick skylark 320
    18.0,   8,        318.0, 150.0, 3436.0, 11.0,   70, plymouth satellite
    16.0,   8,        304.0, 150.0, 3433.0, 12.0,   70, amc rebel sst
    17.0,   8,        302.0, 140.0, 3449.0, 10.5,   70, ford torino
    15.0,   8,        429.0, 198.0, 4341.0, 10.0,   70, ford galaxie 500
    14.0,   8,        454.0, 220.0, 4354.0,  9.0,   70, chevrolet impala
    14.0,   8,        440.0, 215.0, 4312.0,  8.5,   70, plymouth fury iii
    14.0,   8,        455.0, 225.0, 4425.0, 10.0,   70, pontiac catalina
    15.0,   8,        390.0, 190.0, 3850.0,  8.5,   70, amc ambassador dpl
\end{Verbatim}
\vspace*{-0.3cm}
\caption{The head of the file \texttt{cars.csv}.}
\label{fig:cars.csv}
\end{figure}

The file ``\texttt{cars.csv}'' is part of the data set accompanying the excellent book 
\href{http://www-bcf.usc.edu/~gareth/ISL/index.html}{Introduction to Statistical Learning} by Gareth James,
Daniela Witten, Trevor Hastie, and Robert Tibshirani \cite{james:2014}.  The file 
``\texttt{cars.csv}''  contains the fuel consumption of a number of different cars that were in widespread use during
the seventies and early eighties of the last century.  The first column of this data set list the 
\blue{miles per gallon}, i.e.~the number of miles a car can go with one gallon of gas.  Note that
this number is inverse to the fuel consumption:  If a car $\mathrm{A}$ can go twice as many miles per gallon
than another car $\mathrm{B}$, then the fuel consumption of $\mathrm{A}$ is half of the fuel consumption of
$\mathrm{B}$. Furthermore, besides the miles per gallon, for every car the following other parameters are listed:
\begin{enumerate}
\item $\mathtt{cyl}$ is the number of cylinders,
\item $\mathtt{displacement}$ is the engine displacement in cubic inches, 
\item $\mathtt{hp}$ is the engine power given in units of \href{https://en.wikipedia.org/wiki/Horsepower}{horsepower},
\item $\mathtt{weight}$ is the weight in pounds,
\item $\mathtt{acc}$ is the acceleration given as the time in seconds needed to accelerate from 0 miles per
      hour to 60 miles per hour,
\item $\mathtt{year}$ is the year in which the model was introduced, and
\item $\mathtt{name}$ is the name of the model.
\end{enumerate}
Our aim is to determine which of these parameters can be used best to explain the fuel consumption of a car.  To this
end, I have written the program shown in \myFig{simple-linear-regression.stlx}.


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    simple_linear_regression := procedure(fileName, types, name) {
        csv    := readTable(fileName, types);
        data   := csv.getData();
        number := #data;
        index  := find_index(name, csv.getColumnNames());
        Y      := [];
        X      := [];
        L      := [1 .. number];
        for (i in L) {
            Y[i] := 1 / data[i][1];
            X[i] := data[i][index];
        }
        xMean  := +/ X / number;
        yMean  := +/ Y / number;
        theta1 :=   (+/ [(X[i]-xMean)*(Y[i]-yMean) : i in L]) 
                  / (+/ [(X[i]-xMean)**2 : i in L]);
        theta0 := yMean - theta1 * xMean;
        TSS    := +/ [(Y[i] - yMean) ** 2 : i in L];
        RSS    := +/ [(Y[i] - theta0 - theta1 * X[i]) ** 2 : i in L];
        R2     := 1 - RSS / TSS;
        canvas := plot_createCanvas("Fuel consumption vs. $name$");
        plot_addBullets(canvas, [ [X[i], Y[i]] : i in L ], [0,0,255], 2.0);
        return R2;
    };
    find_index := procedure(x, List) {
        return arb({ idx : idx in [1 .. #List] | List[idx] == x });
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Simple Linear Regression}
\label{fig:simple-linear-regression.stlx}
\end{figure}


\noindent
The procedure $\mathtt{simple\_linear\_regression}$ take three arguments:
\begin{enumerate}[(a)]
\item $\mathtt{fileName}$ is the name of the file containing the data.

      It is assumed that this file is a \textsc{Csv} file containing the data that need to be analyzed.
\item $\mathtt{types}$ is a list of type names of the columns present in the \texttt{\textsc{Csv}} file.  This is needed
      by \textsc{SetlX} in order to correctly interpret the data.

\item $\mathtt{name}$ is the column name that is to be used for linear regression, i.e.~it specifies the column that 
      contains the values for the independent variable $x$.  The dependent variable $y$ is assumed to be given in
      the first column of the \textsc{Csv} file.
\end{enumerate}
The implementation of the procedure $\mathtt{simple\_linear\_regression}$ works as follows:
\begin{enumerate}
\item The function \texttt{readTable} is used to read the file specified in $\mathtt{fileName}$.
      It returns the object $\mathtt{csv}$ which is of class $\mathtt{Table}$.  This class and the implementation of the function
      $\mathtt{readTable}$ is shown in \myFig{table.stlx}.  The object $\mathtt{csv}$ contains both the column
      names as well as the data that is present in the given file.  The list $\mathtt{types}$ is needed in
      order to convert the strings that are read into their proper data types.
\item The function $\mathtt{getData}$ extracts the $\mathtt{data}$ from the object $\mathtt{csv}$.
      Technically, $\mathtt{data}$ is a list of lists.  The list $\mathtt{data}$ has the same length as there are lines
      in the file specified by $\mathtt{fileName}$.  Every single line in this file is converted
      into a list of entries of the appropriate types.  These lists are the elements of the list $\mathtt{data}$.
\item $\mathtt{number}$ is the number of data lines in the file specified by $\mathtt{fileName}$,
      i.e.~$\mathtt{number}$ is what we have called $m$ in the formul\ae\ given previously.
\item $\mathtt{index}$ is the index of the column that has the given $\mathtt{name}$.
      For example, in the case of the file shown in \myFig{cars.csv}, the $\mathtt{index}$ of the
      $\mathtt{name}$ ``$\mathtt{displacement}$'' is $3$ as the engine displacement is given in the third
      column of the file ``\texttt{cars.csv}''.
\item The values of the dependent variable $y$ are stored in the list $\mathtt{Y}$ and the values of the
      independent variable $x$ are stored in the list $\mathtt{X}$.
\item The list $\mathtt{L}$ is used as an abbreviation so that iterations over all lists stored in
      $\mathtt{data}$ are simplified.
\item The \texttt{for}-loop fills the lists $\mathtt{X}$ and $\mathtt{Y}$.  Since the file
      ``\texttt{cars.csv}'' contains the miles per gallon in the first column labelled $\mathtt{mpg}$,
      we need to convert this data into fuel consumption.  As the fuel consumption is the reciprocal of the miles per
      gallon, the list $\mathtt{Y}$ is filled with the reciprocal of the first column of the file
      ``\texttt{cars.csv}''. 
\item $\mathtt{xMean}$ is the mean value $\bar{x}$ of the independent variable $\mathbf{x}$.
\item $\mathtt{yMean}$ is the mean value $\bar{\mathbf{y}}$ of the dependent variable $\mathbf{y}$. 
\item The coefficient $\mathtt{theta1}$ is computed according to Equation \ref{eq:theta1}, which is repeated
      here for convenience:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\vartheta_1 = \frac{\sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)}{
                        \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2}  
      $.
\item The coefficient $\mathtt{theta0}$ is computed according to Equation \ref{eq:theta0}, which reads
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}$.
\item $\mathtt{TSS}$ is the \blue{total sum of squares} and is computed using the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\mathtt{TSS} = \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2$.
\item $\mathtt{RSS}$ is the \blue{residual sum of squares} and is computed as
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\mathtt{RSS} := \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2$.
\item $\mathtt{R2}$ is the $\mathtt{R}^2$ statistic and measures the \blue{proportion of the explained variance}.
      It is computed using the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \mathtt{R}^2 = \frac{\mathtt{TSS} - \mathtt{RSS}}{\mathtt{TSS}}$.
\item The function $\mathtt{plot\_createCanvas}$ creates a $\mathtt{canvas}$ that is used for plotting.
\item The function $\mathtt{plot\_addBullets}$ plots data point onto the $\mathtt{canvas}$.
      The second argument to this function is a list of pairs of the form
      \\[0.2cm]
      \hspace*{1.3cm}
      $[ \pair(x_1,y_1), \cdots, \pair(x_m,y_m)]$
      \\[0.2cm]
      The pair $\pair(x_i,y_1)$ is then plotted as a blue circle with radius $2.0$ at the position
      $\pair(x_i, y_i)$.
\end{enumerate}
\myFig{table.stlx} shows the implementation of the class table and the function $\mathtt{readTable}$.  This figure is
only given for completeness.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    class Table(columnNames, types, data) {
        mColumnNames := columnNames;
        mTypes       := types;
        mData        := data;
      static {
          getColumnNames := [ ] |-> mColumnNames;
          getTypes       := [ ] |-> mTypes;
          getData        := [ ] |-> mData;
          getRow         := [r] |-> mData[r];
          getLength      := [ ] |-> #mData;
          
          head := procedure(limit := 10) {
              print(mColumnNames);
              print(mTypes);
              for (i in [1 .. limit]) {
                  print(mData[i]);
              }
          };
      }
    }
    readTable := procedure(fileName, types) {
        allData     := readFile(fileName);  // list of lines
        columnNames := split(allData[1], ',\s*');
        data        := [];
        for (i in [2 .. #allData]) {
            row       := split(allData[i], ',\s*');
            data[i-1] := [evalType(type, s) : [type, s] in types >< row];
        }
        return Table(columnNames, types, data);
    };
    evalType := procedure(type, s) {
        switch {
            case     type == "double": return double(s);
            case     type == "int"   : return int(s);
            case     type == "string": return s;
            default: abort("unknown type $type$ in evalType");
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{The class $\mathtt{Table}$.}
\label{fig:table.stlx}
\end{figure}


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    test := procedure() {
      types := [ "double", "int", "double", "double",
                 "double", "double", "int", "string"
               ];
        for (varName in ["displacement", "cyl", "hp", "weight", "acc", "year"]) {
            R2 := simple_linear_regression("cars.csv", types, varName);
            print("Explained variance for $varName$ vs fuel consumption: $R2$."); 
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Calling the procedure $\mathtt{simple\_linear\_regression}$.}
\label{fig:simple-linear-regression.stlx:test}
\end{figure}

\myFig{simple-linear-regression.stlx:test} shows how to call the procedure
$\mathtt{simple\_linear\_regression}$.  We need to define the types of the various columns that are present in
the file ``\texttt{cars.csv}''.  Next, for all those column names present in this file, we use this variable to
compute the explained variance.  The resulting values are shown in Table \ref{tab:explained-variance}.  It
seems that, given the data in the file ``\texttt{cars.csv}'', the best indicator for the fuel consumption is
the $\mathtt{weight}$ of a car.  The $\mathtt{displacement}$, the power $\mathtt{hp}$ of an engine, and the
number of cylinders $\mathtt{cyl}$ are also
good predictors.  But notice that the $\mathtt{weight}$ is the real cause of fuel consumption:  If a car
has a big weight, it will also need a more powerful engine.  Hence the variable $\mathtt{hp}$ is correlated
with the variable $\mathtt{weight}$ and will therefore also provide a reasonable explanation of the fuel
consumption, although the high engine power is  not the most important cause of the fuel consumption.


\begin{table}
  \centering
  \begin{tabular}{|l|r|}
  \hline
  dependent variable      & explained variance   \\
  \hline
  \hline
  $\mathtt{displacement}$ & 0.75                 \\
  \hline
  $\mathtt{cyl}$          & 0.70                 \\
  \hline
  $\mathtt{hp}$           & 0.73                 \\
  \hline
  $\mathtt{weight}$       & 0.78                 \\
  \hline
  $\mathtt{acc}$          & 0.21                 \\
  \hline
  $\mathtt{year}$         & 0.31                 \\
  \hline
  \end{tabular}
  \caption[explained variance]{Explained variance for various dependent variables.}
  \label{tab:explained-variance}
\end{table}
\pagebreak


\subsection{Testing the Statistical Significance}
In this section we answer the question of how to assess the 
\href{https://en.wikipedia.org/wiki/Statistical_significance}{statistical significance} of our
results.  In the case of simple linear regression, the
\href{https://en.wikipedia.org/wiki/Null_hypothesis}{null hypothesis} is given as follows: 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{H}_0$: There is no relationship between the variables $x$ and $y$.
\\[0.2cm]
In order to compute the probability that the null hypothesis is true, we have to compute the
\blue{ $t$-statistic}, which is given by the following formula:
\begin{equation}
  \label{eq:t-statistics}
  t := |\vartheta_1| \cdot \sqrt{\frac{\;(m-2)\;}{\mathtt{RSS}} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2\; }.
\end{equation}
The $t$ statistics is distributed according to 
\href{https://en.wikipedia.org/wiki/Student%27s_t-distribution}{Student's $t$-distribution} 
  with $m-2$ degrees of freedom.
The cumulative Student's $t$-distribution can be computed via the function
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{stat\_studentCDF}(t, \nu)$,
\\[0.2cm]
where $t$ is the value of the $t$-statistic and $\nu$ is the number of degrees of freedom.  If we substitute
$\nu := m - 2$, then the expression 
\\[0.2cm]
\hspace*{1.3cm}
$1 - \mathtt{stat\_studentCDF}(t, \nu)$,
\\[0.2cm]
 computes the probability that the null hypothesis is true.  This probability
is also known as the \blue{$p$-value}.  If the $p$-value is small, e.g.~less than $0.01$, then the null
hypothesis is refuted and we can conclude that there is some relationship between $x$ and $y$.  In the example
previously discussed, i.e.~the example relating the fuel consumption of a car to various other parameters, the
$p$-values are all very small, i.e.~less than $2 \cdot 10^{-16}$.  This is due to the fact that we have had enough data at
our disposal:  As a rule of thumb it can be said that once we have more than 30 pairs $\pair(x_i, y_i)$ and there
is indeed a relationship between $x$ and $y$, then simple linear regression will detect this relationship.

\section{General Linear Regression}
In practise, it is rarely the case that a given observed variable $y$ only depends on a single variable $x$.
To take the example of the fuel consumption of a car further, in general we would expect that the fuel consumption
of a car depends not only on the mass of the car but is also related to the other parameters.  
To this end, we present the theory of   
\href{https://en.wikipedia.org/wiki/Linear_regression}{general linear regression}.
In a \blue{general regression problem} we are given a list of $m$ pairs of the form $\langle\mathbf{x}^{(i)}, y^{(i)} \rangle$ 
where $\mathbf{x}^{(i)} \in \mathbb{R}^p$ and $y^{(i)} \in \mathbb{R}$ for all $i \in \{1,\cdots,m\}$.  The
number $p$ is called the number of \blue{features}, while the pairs are called the \blue{training examples}.
Our goal is to compute a function 
\\[0.2cm]
\hspace*{1.3cm}
$F:\mathbb{R}^p \rightarrow \mathbb{R}$
\\[0.2cm]  
such that $F\bigl(\mathbf{x}^{(i)}\bigr)$ approximates  $y^{(i)}$ as precisely as posssible
for all $i\in\{1,\cdots,m\}$, i.e.~we want to have
\\[0.2cm]
\hspace*{1.3cm}
$\forall i\in\{1,\cdots,m\}:F\bigl(\mathbf{x}^{(i)}\bigr) \approx y^{(i)}$.
\\[0.2cm]
In order to make the notation $F\bigl(\mathbf{x}^{(i)}\bigr) \approx y^{(i)}$ more precise, we
define the \blue{mean squared error} 
\begin{equation}
  \label{eq:squared-error-1}
  \mathtt{MSE} := \frac{1}{m-1} \cdot \sum\limits_{i=1}^{m} \Bigl(F\bigl(\mathbf{x}^{(i)}\bigr) - y^{(i)}\Bigr)^2. 
\end{equation}
Then, given the list of training examples $[\langle \mathbf{x}^{(1)}, y^{(1)} \rangle, \cdots, \langle
\mathbf{x}^{n}, y^{(n)} \rangle]$, our goal is to minimize $\mathtt{MSE}$.  
In order to proceed, we need to have a model for the function $F$.  The simplest model is a linear
model, i.e.~we assume that $F$ is given as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds F(\mathbf{x}) = \sum\limits_{j=1}^p w_j \cdot x_j + b = \mathbf{x}^\top \cdot \mathbf{w} + b$ \quad where $\mathbf{w} \in \mathbb{R}^p$ and $b\in\mathbb{R}$.
\\[0.2cm]
Here, the expression $\mathbf{x}^\top \cdot \mathbf{w}$ denotes the matrix product of the vector
$\mathbf{x}^\top$, which is viewed as a $1$-by-$m$ matrix, and the vector $\mathbf{w}$.  Alternatively, this
expression could be interpreted as the dot product of the vector $\mathbf{x}$ and the vector $\mathbf{w}$.
At this point you might wonder why it is useful to introduce matrix notation here.  The reason is
that this notation shortens the formula and, furthermore, is more efficient to implement since most
programming languages used in machine learning have special library support for matrix operations.  
Provided the computer is equipped with a graphics card,  some
programming languages are even able to delegate matrix operations to the graphics unit.  This results in a
considerable speed-up.

The definition of $F$ given above is the model used in
\href{https://en.wikipedia.org/wiki/Linear_regression}{linear regression}. 
Here, $\mathbf{w}$ is called the \blue{\color{blue}weight vector} and $b$ is called the \blue{\color{blue}bias}.  It turns
out that the notation can be simplified if we extend the $p$-dimensional feature vector $\mathbf{x}$ to an
$p+1$-dimensional vector $\mathbf{x}'$ such that
\\[0.2cm]
\hspace*{1.3cm}
$x_j' := x_j$ \quad for all $j\in\{1,\cdots,p\}$ \quad and \quad $x_{m+1}' := 1$.
\\[0.2cm]
To put it in words, the vector $\mathbf{x}'$ results from the vector $\mathbf{x}$ by appending the number $1$:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}' = \langle x_1, \cdots, x_p, 1 \rangle^\top$ \quad where $\langle x_1, \cdots, x_p \rangle = \mathbf{x}^\top$.
\\[0.2cm]
Furthermore, we define 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}' := \langle w_1, \cdots, w_p, b \rangle^\top$ \quad where $\langle w_1, \cdots, w_p \rangle = \mathbf{w}^\top$.
\\[0.2cm]
Then we have
\\[0.2cm]
\hspace*{1.3cm}
$F(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b = \mathbf{w}' \cdot \mathbf{x}'$.
\\[0.2cm]
Hence, the bias has been incorporated into the weight vector at the cost of appending the number $1$ at the end of
input vector.  As we want to use this simplification, from now on we assume that the input vectors
$\mathbf{x}^{(i)}$ have all been extended so that their last component is $1$.  Using this
assumption,  we define the
function $F$ as
\\[0.2cm]
\hspace*{1.3cm}
$F(\mathbf{x}) := \mathbf{x}^\top \cdot \mathbf{w}$.
\\[0.2cm]
Now equation (\ref{eq:squared-error-1}) can be rewritten as follows:
\begin{equation}
  \label{eq:squared-error-2}
  \mathtt{MSE}(\mathbf{w}) = \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \Bigl(\bigl(\mathbf{x}^{(i)})^\top \cdot \mathbf{w}  - y^{(i)}\Bigr)^2.
\end{equation}
Our aim is to rewrite the sum appearing in this equation as a scalar product of a vector with
itself.  To this end, we first define the vector $\mathbf{y}$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{y} := \langle y^{(1)}, \cdots, y^{(m)} \rangle^\top$.
\\[0.2cm]
Note that $\mathbf{y} \in \mathbb{R}^m$ since it has a component for all of the $m$ training
examples.  Next, we define the \blue{design matrix} $X$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$X := \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^\top  \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^\top
  \end{array}
  \right)   
$
\\[0.2cm]
Defined this way, the row vectors of the matrix $X$ are the vectors $\mathbf{x}^{(i)}$ transposed.
Now we have the following:
\\[0.2cm]
\hspace*{1.3cm}
$X \cdot \mathbf{w} - \mathbf{y} = \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^\top  \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^\top
  \end{array}
  \right) \cdot \mathbf{w} - \mathbf{y} = \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^\top \cdot \mathbf{w} - y_1 \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^\top \cdot \mathbf{w} - y_m
  \end{array}
  \right)
$
\\[0.2cm]
Taking the square of the vector $X \cdot \mathbf{w} - \mathbf{y}$ we discover that
we can rewrite equation (\ref{eq:squared-error-2}) as follows:
\begin{equation}
  \label{eq:squared-error-3}
  \mathtt{MSE}(\mathbf{w}) = \frac{1}{m-1} \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr)^\top \cdot 
                                            \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr).
\end{equation}

\subsection{Some Useful Gradients}
In the last section, we have computed the mean squared error $\mathtt{MSE}(\mathbf{w})$ using equation
(\ref{eq:squared-error-3}).  Our goal is to minimize the $\mathtt{MSE}(\mathbf{w})$ by choosing the weight
vector $\mathbf{w}$ appropriately.  A necessary condition for $\mathtt{MSE}(\mathbf{w})$ to be minimal is 
\\[0.2cm]
\hspace*{1.3cm}
$\nabla \mathtt{MSE}(\mathbf{w}) = \mathbf{0}$,
\\[0.2cm]
i.e.~the gradient of $\mathtt{MSE}(\mathbf{w})$ with respect to $\mathbf{w}$ needs to be zero.  In order to prepare for the computation of
$\nabla \mathtt{MSE}(\mathbf{w})$, we first compute the gradient of two simpler functions.

\subsubsection{Computing the Gradient of $f(\mathbf{x}) = \mathbf{x}^\top \cdot C \cdot \mathbf{x}$}
Suppose the function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is defined as
\\[0.2cm]
\hspace*{1.3cm}
$f(\mathbf{x}) := \mathbf{x}^\top \cdot C \cdot \mathbf{x}$ \quad where $C \in \mathbb{R}^{n \times n}$.
\\[0.2cm]
If we write the matrix $C$ as $C = (c_{i,j})_{i=1,\cdots,n \atop j=1,\cdots,n}$ and the vector
$\mathbf{x}$ as $\mathbf{x} = \langle x_1, \cdots, x_n \rangle^\top$,  then $f(\mathbf{x})$ can be
computed as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(\mathbf{x}) = \sum\limits_{i=1}^n x_i \cdot \sum\limits_{j=1}^n c_{i,j} \cdot x_j 
                   = \sum\limits_{i=1}^n \sum\limits_{j=1}^n x_i \cdot c_{i,j} \cdot x_j
$.
\\[0.2cm]
We compute the partial derivative of $f$ with respect to $x_k$ and use the product rule together with the
definition of the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta} $\delta_{i,j}$, which
is defined as $1$ if $i = j$ and as $0$ otherwise:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
\ds \frac{\partial f}{\partial x_k} & = &
\ds \sum\limits_{i=1}^n \sum\limits_{j=1}^n \Bigl(
    \frac{\partial x_i}{\partial x_k} \cdot c_{i,j} \cdot x_j + x_i \cdot c_{i,j} \cdot \frac{\partial x_j}{\partial x_k}
    \Bigr) \\[0.5cm]
& = &
\ds \sum\limits_{i=1}^n \sum\limits_{j=1}^n \Bigl(
    \delta_{i,k} \cdot c_{i,j} \cdot x_j + x_i \cdot c_{i,j} \cdot \delta_{j,k} \Bigr) \\[0.5cm]
& = &
\ds \sum\limits_{j=1}^n c_{k,j} \cdot x_j + \sum\limits_{i=1}^n x_i \cdot c_{i,k} \\[0.5cm]
& = &
  \bigl(C \cdot \mathbf{x}\bigr)_k + \bigl(C^\top \cdot \mathbf{x}\bigr)_k
\end{array}
$
\\[0.2cm]
Hence we have shown that 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla f(\mathbf{x}) = (C + C^\top) \cdot \mathbf{x}$.
\\[0.2cm]
If the matrix $C$ is \blue{symmetric}, i.e.~if $C = C^\top$, this simplifies to
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla f(\mathbf{x}) = 2 \cdot C \cdot \mathbf{x}$.
\\[0.2cm]
Next, if the function $g: \mathbb{R}^n \rightarrow \mathbb{R}$ is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$g(\mathbf{x}) := \mathbf{b}^\top \cdot A \cdot \mathbf{x}$, \quad where $\mathbf{b} \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$,
\\[0.2cm]
then a similar calculation shows that
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla g(\mathbf{x}) = A^\top \cdot \mathbf{b}$.

\exercise
Prove this equation.

\subsection{Deriving the Normal Equation}
Next, we derive the so called \blue{normal equation} for linear regression.  To this end, we first
expand the product in equation (\ref{eq:squared-error-3}):
\\[0.2cm]
\hspace*{0.3cm}
$
\begin{array}[t]{lcll}
 \mathtt{MSE}(\mathbf{w}) & = & 
 \ds \frac{1}{m-1} \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr)^\top \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr) 
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\top \cdot X^\top - \textbf{y}^\top\bigr) \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr) 
 & \mbox{since $(A \cdot B)^\top = B^\top \cdot A^\top$}
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\top \cdot X^\top \cdot X \cdot \mathbf{w} 
                             - \textbf{y}^\top \cdot X \cdot \mathbf{w} 
                             - \mathbf{w}^\top \cdot X^\top \cdot \mathbf{y}
                             + \mathbf{y}^\top \cdot \mathbf{y}
                       \bigr)
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\top \cdot X^\top \cdot X \cdot \mathbf{w} 
                             - 2 \cdot \textbf{y}^\top \cdot X \cdot \mathbf{w} 
                             + \mathbf{y}^\top \cdot \mathbf{y}
                       \bigr)
 & \mbox{since $\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y} = \textbf{y}^\top \cdot X \cdot \mathbf{w}$}
\end{array}
$
\\[0.2cm]
The fact that 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y} = \textbf{y}^\top \cdot X \cdot \mathbf{w}$
\\[0.2cm]
might not be immediately obvious.  It follows from two facts:
\begin{enumerate}
\item For two matrices $A$ and $B$ such that the matrix product $A \cdot B$ is defined we have 
      \\[0.2cm]
      \hspace*{1.3cm}
      $(A \cdot B)^\top = B^\top \cdot A^\top$.
\item The matrix product $\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y}$ is a real number.  The transpose $r^\top$ of a real number $r$ is the number
      itself, i.e.~$r^\top = r$ for all $r \in \mathbb{R}$.  Therefore, we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y} = 
\bigl(\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y}\bigr)^\top =
\mathbf{y}^\top \cdot X \cdot \mathbf{w}
$.
\end{enumerate}
Hence we have shown that
\begin{equation}
  \label{eq:squared-error-4}
  \mathtt{MSE}(\mathbf{w}) = \ds \frac{1}{m-1} \cdot \Bigl(\mathbf{w}^\top \cdot \bigl(X^\top \cdot X\bigr) \cdot \mathbf{w} 
                                             - 2 \cdot \textbf{y}^\top \cdot X \cdot \mathbf{w} 
                                             + \mathbf{y}^\top \cdot \mathbf{y}
                                        \Bigr)
\end{equation}
holds.  The matrix $X^\top \cdot X$ used in the first term is symmetric because
\\[0.2cm]
\hspace*{1.3cm}
$\bigl(X^\top \cdot X\bigr)^\top = X^\top \cdot \bigl(X^\top\bigr)^\top = X^\top \cdot X$.
\\[0.2cm]
Using the results from the previous section we can now compute the gradient of $\mathtt{MSE}(\mathbf{w})$ with respect to
$\mathbf{w}$.  The result is
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla \mathtt{MSE}(\mathbf{w}) = \frac{2}{m-1} \cdot \Bigl(X^\top \cdot X \cdot \mathbf{w} - X^\top \cdot \mathbf{y}\Bigr)$.
\\[0.2cm]
If the squared error $\mathtt{MSE}(\mathbf{w})$ has a minimum for the weights $\mathbf{w}$, then we must have
\\[0.2cm]
\hspace*{1.3cm}
$\nabla \mathtt{MSE}(\mathbf{w}) = \mathbf{0}$.
\\[0.2cm]
This leads to the equation
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{2}{m-1} \cdot \Bigl(X^\top \cdot X \cdot \mathbf{w} - X^\top \cdot \mathbf{y}\Bigr) = \mathbf{0}$.
\\[0.2cm]
This equation can be rewritten as
\begin{equation}
  \label{eq:normal-equation}
 \colorbox{red}{\framebox{\colorbox{yellow}{\framebox{
 $\ds\bigl(X^\top \cdot X\bigr) \cdot \mathbf{w} = X^\top \cdot \mathbf{y}$.}}}} 
\end{equation}
This equation is called the \blue{normal equation}.  

\remark
Although the matrix $X^\top \cdot X$ will often be invertible, for numerical reasons it is not
advisable to rewrite the normal equation as
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w} = \bigl(X^\top \cdot X)^{-1} \cdot X^\top \cdot \mathbf{y}$.
\\[0.2cm]
Instead, when solving the normal equation we will use the \textsc{SetlX} function $\mathtt{la\_solve}(A,b)$, which
takes a matrix $A \in \mathbb{R}^{n \times n}$ and a vector $\mathbf{b} \in \mathbb{R}^n$ and solves the equation
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \mathbf{x} = \mathbf{b}$.  \eox


\subsection{Testing the Statistical Significance}
The $\mathrm{F}$-statistic is defined according to the following formula:
\begin{equation}
  \label{eq:F-statistic}
  \mathrm{F} = \frac{\mathtt{TSS} - \mathtt{RSS}}{\mathtt{RSS}} \cdot \frac{m - p - 1}{p}
\end{equation}
The $\mathtt{F}$-statistic is distributed according to the
\href{https://en.wikipedia.org/wiki/F-distribution}{Fisher-Snedecor-distribution} with $p-1$ degrees of freedom
in the nominator and $m - p$ degrees of freedom in the denominator. 

\subsection{Implementation}
\myFig{linear-regression.stlx} shows an implementation of general linear regression.
The procedure
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{run\_linear\_regression}(f, t)$
\\[0.2cm]
takes a file name $f$ and a list of types $t$ as its input.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    run_linear_regression := procedure(fileName, types) {
        csv    := readTable(fileName, types);
        data   := csv.getData();
        number := #data;
        xList  := [];
        yList  := [];
        for (i in [1 .. number]) {
            yList[i] := 1 / data[i][1];
            xList[i] := la_vector(data[i][2..-2] + [1.0]);
        }
        X := la_matrix(xList);
        y := la_vector(yList);
        w := la_solve(X! * X, X! * y);
        d := X * w - y;
        RSS   := d * d;
        yMean := +/ yList / number;
        TSS   := +/ [(yList[i] - yMean) ** 2 : i in [1 .. number]];
        R2    := 1 - RSS / TSS;
        return R2;
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{General linear regression.}
\label{fig:linear-regression.stlx}
\end{figure}
\begin{enumerate}
\item It reads the file and stores the resulting $\mathtt{Table}$ object in the variable $\mathtt{csv}$.
\item The data from this object is stored as a list of lists in the variable $\mathtt{data}$.
\item $\mathtt{number}$ is the number of training examples.
\item $\mathtt{xList}$ stores the data of the independent variable $x$.  Therefore, $\mathtt{xList}$ is a list
      of vectors.  Note that we have extended the data vectors by putting the number $1.0$ at the end of every vector.  
\item $\mathtt{yList}$ stores the data of the depend variable $y$.
\item The data in $\mathtt{xList}$ is then stored in the matrix $\mathtt{X}$.  
\item The data in $\mathtt{yList}$ is stored in the vector $\mathtt{y}$.
\item The normal equation is formulated and solved using the function $\mathtt{la\_solve}$.
      Note that the postfix-operator ``\texttt{!}'' computes the transpose of a matrix when applied to a matrix.
\item The variable $\mathtt{d}$ is the difference between the predictions of the linear model and the observed
      values $\mathtt{y}$.
\item $\mathtt{RSS}$ is the residual sum of squares.
\item $\mathtt{yMean}$ is the mean value of the variable $y$.
\item $\mathtt{TSS}$ is the total sum of squares.
\item $\mathtt{R2}$ is the proportion of the explained variance.
\end{enumerate}



When we run the program shown in \myFig{linear-regression.stlx} with the data stored in \texttt{cars.csv},
which had been discussed previously, then the proportion of explained variance is $81 \mathtt{\symbol{37}}$.  Considering that our data does
not take account of the aerodynamics of the cars, this seems like a reasonable result.


\exercise
The file ``\texttt{advertising.csv}'', which is available at
\\[0.2cm]
\hspace*{1.3cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/SetlX/advertising.csv}{\texttt{https://github.com/karlstroetmann/Artificial-Intelligence/\\
\hspace*{1.22cm} blob/master/SetlX/advertising.csv}},
\\[0.2cm]
contains data about advertising.  Concretely, let us think that this data lists the amount of money that has
been spent in advertising a luxury refrigerator that is able to detect the amount of food stored and that is
able to reorder supplies with \href{https://en.wikipedia.org/wiki/AmazonFresh}{AmazonFresh}$^\mathtt{TM}$.
Every line of this \textsc{Csv} file has five entries:
\begin{enumerate}[(a)]
\item The first column is interpreted as the number of a store.  All together, there are 200 stores that have sold this
      refrigerator.  It is assumed that all of these stores are located in different counties.
\item The second column gives the amount of money spent in TV advertising in thousands of dollar for the store in
      that county.
\item The third column gives the amount of money spent in radio advertising in thousands of dollar.
\item The fourth column gives the amount of money spent in newspaper advertising in thousands of dollar.
\item The fifth column gives the number of refrigerators in thousands that have been sold in the corresponding store.
\end{enumerate}
With the exception of the first column, all columns contain floating point numbers.
Assume you are the marketing manager that is responsible for the advertising campaign.  Your task is to
evaluate the effectiveness of the different type of advertisements.  To this end, answer the following questions.
\begin{enumerate}
\item Should you spend more money on newspaper adds and less on TV adds or is it the other way around?  Or are radio
      advertisements the way to go? Assume that your advertising budget is fixed and that you want to sell as many
      refrigerators as possible.  

      Use linear regression to solve this question.
\item At this point, you will have discovered that only two of the three different types of advertisements 
      contribute to the number of sales.  Check, whether there is an interaction between these two types of
      advertisements.  For example, check whether TV adds have a positive or negative effect on the
      effectiveness of radio adds. 

      You can use a product term as a new features in your linear regression model to answer this question.
      For example, you can include the product of money spend in TV adds and money spend in radio adds as a new
      feature.
\item In order to further increase the accuracy of our model, we can use a \blue{quadratic model}.
      If we have only two independent features $x$ and $y$, a quadratic model has the following form:
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(x,y) = w_1 \cdot x^2 + w_2 \cdot x \cdot y + w_3 \cdot y^2 + w_4 \cdot x + w_5 \cdot y + w_6$.
      \\[0.2cm]
      Compute the fraction of the explained variance that is achieved if a quadratic model is used to predict
      the sales.
\item In the previous part of the exercise you will have discovered that for a quadratic model, the fraction of
      the explained variance is quite high.  Hence, the quadratic model seems to be accurate.  Now that you
      have found an accurate model, by how much can you increase the sales by optimizing the strategy?
      Assume that for every county the company wants to spend the same amount of money for advertisement that
      has been spent previously for that county.  
\end{enumerate}
\vspace*{0.2cm}

\noindent
\textbf{Note}:  I have stolen the file ``\texttt{advertising.csv}'' from the data set accompanying the book
``\href{http://www-bcf.usc.edu/~gareth/ISL/index.html}{Introduction to Statistical Learning}''
\cite{james:2014}.  As far as I understand it, the data contained in this file is fictional data, i.e.~it
has been made up. 
\eox

\noindent
\textbf{Partial Solution to part 4:}  
The total budget for advertising in a given county is fixed.  Assume
this total budget is called $b$.  If $x$ is the amount of money spent for TV advertisements and $y$ is the
amount of money spent for radio advertisements, then we must have
\\[0.2cm]
\hspace*{1.3cm}
$x + y = b$ \quad and therefore \quad $y = b - x$.
\\[0.2cm]
Hence the number of units sold is given by the formula
\\[0.2cm]
\hspace*{1.3cm}
$g(x) := f(x, b - x) = w_1 \cdot x^2 + w_2 \cdot x \cdot (b - x) + w_3 \cdot (b - x)^2 + w_4 \cdot x + w_5 \cdot (b - x) + w_6$.
\\[0.2cm]
In order to compute the value $x$ that maximizes $g$ we have to compute the derivative of $g(x)$:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
\ds \frac{\mathrm{d} g}{\mathrm{d}x}(x) 
& = & 2 \cdot w_1 \cdot x + w_2 \cdot b - 2 \cdot w_2 \cdot x + 2 \cdot w_3 \cdot (x - b) + w_4 - w_5 \\[0.2cm]
& = & 2 \cdot \bigl(w_1 - w_2 + w_3\bigr) \cdot x + w_2 \cdot b - 2 \cdot w_3 \cdot b + w_4 - w_5  
\end{array}
$
\\[0.2cm]
This expression is $0$ if and only if
\\[0.2cm]
\hspace*{1.3cm}
$\ds x = \frac{- w_2 \cdot b + 2 \cdot w_3 \cdot b - w_4 + w_5}{2 \cdot (w_1 - w_2 + w_3)}$.
\\[0.2cm]
It turns out that when the advertising strategy is optimized as outline above,  the quadratic model predicts a
sale of $5\,511\,887$ units, while the number of units sold previously was $2\,804\,500$ units.  Hence, the
optimized strategy is expected to sell about $97\,\mathtt{\symbol{37}}$ units more than the strategy applied previously. 
\qed




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "artificial-intelligence"
%%% End:
