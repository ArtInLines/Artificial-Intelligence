{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100%; !important } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container { width:100%; !important } </style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Natural Language Toolkit (NLKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the <a href=\"https://www.nltk.org\">Natural Language ToolKit</a> (NLKT).  Our first example is concerned with <em style=\"color:blue;\">classification</em>: We want to see whether it is possible to predict the gender of a given first name.  This example is taken from <a href=\"https://www.nltk.org/book/ch06.html\">Chapter 6</a> of the <a href=\"https://www.nltk.org/book\">NLTK book</a>.  To begin with, we import the module `nltk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides a number of builtin datasets.  We will start by importing the object `names` from the module `nltk.corpus`.  The dataset `names` consists of a number of first names for both genders.  To be more precise, `names` is an object of class `nltk.corpus.util.LazyCorpusLoader` that provides methods to load both female and male first names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.util.LazyCorpusLoader"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "type(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load these names into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/karlstroetmann/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of female first names: 5001\n",
      "Number of   male first names: 2943\n"
     ]
    }
   ],
   "source": [
    "FemaleNames = names.words('female.txt')\n",
    "MaleNames   = names.words('male.txt'  )\n",
    "print('Number of female first names:', len(FemaleNames))\n",
    "print('Number of   male first names:', len(MaleNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are more female first names than there are male first names.  Lets take a look at the first 5 female names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FemaleNames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we inspect the male names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaleNames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine these two lists into one list of <em style=\"color:blue;\">tagged names</em>, where a *tagged name* is a pair of the form\n",
    "$$ (\\textrm{name}, \\textrm{gender}) \\quad \\mbox{such that $\\textrm{gender} \\in \\{\\texttt{'f'},\\texttt{'m'}\\}$.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Names = [(n, 'm') for n in MaleNames] + [(n, 'f') for n in FemaleNames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to test whether it is possible to predict the gender of a given name using a <em style=\"color:blue;\">Naive Bayes</em> classifier.  In order to be able to make a quantitative assessment of the <em style=\"color:blue;\">accuracy</em> of the classifier, we have to split our data into a <em style=\"color:blue;\">training</em> dataset and a <em style=\"color:blue;\">testing</em> dataset.  To minimize any bias, the assignment of the names into those datasets should be done <em style=\"color:blue;\">randomly</em>.  In order for our results to be <em style=\"color:blue;\">reproducible</em>, we set a <em style=\"color:blue;\">seed</em> for the random number generator.  This ensures that the random number generator will always behave the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "random.shuffle(Names)\n",
    "len(Names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign the majority of the names to the training set.  Roughly 10% of the data are assigned to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11883182275931521"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = Names[:7000], Names[7000:]\n",
    "len(test_set)/(len(Names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to decide the features that we want to use in order predict the gender of a name.  Our first attempt to predict the gender of a word uses just a single feature. This feature is the substring containing the last two characters of the name.  \n",
    "\n",
    "The <em style=\"color:blue;\">classifiers</em> that are already implemented in `NLTK` assume a special format for the features: The features of an object to be classified have to be implemented as a <em style=\"color:blue;\">dictionary</em>.  The keys of the features are supposed to be short descriptions of the features.  Later, we will try to increase the accuracy of our prediction by adding more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return { 'ending': word[-2:] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function on the name `'Hugo'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ending': 'go'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('Hugo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to transform the names in our training set into features in order to train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'ending': 'ka'}, 'f'),\n",
       " ({'ending': 'sa'}, 'f'),\n",
       " ({'ending': 'tt'}, 'f'),\n",
       " ({'ending': 'ie'}, 'f'),\n",
       " ({'ending': 'na'}, 'f'),\n",
       " ({'ending': 'by'}, 'm'),\n",
       " ({'ending': 'as'}, 'f'),\n",
       " ({'ending': 'la'}, 'f'),\n",
       " ({'ending': 'ew'}, 'm'),\n",
       " ({'ending': 'dy'}, 'f')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_features = [(gender_features(n), g) for (n, g) in train_set]\n",
    "train_set_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our first classifier.  To begin with, we use a `NaiveBayesClassifier`, which is already predefined in the module `nltk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether this classifier can predict the gender of the name `Hugo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Hugo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has correctly predicted the gender of `'Hugo'` to be *male*.  But before we get too excited, we should check the accuracy of the classifier on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8044285714285714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, train_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this is our first attempt, an accuracy of 80% is not too bad. After all, so far we are using just a single feature.  The question is, whether our classifier is able to <em style=\"color:blue;\">generalize</em> its predictions to examples it has not seen before.  In order to answer this question we have to use the <em style=\"color:blue;\">test set</em>.  Again, we first have to transform the names from the test set into features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7807203389830508"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_features = [(gender_features(n), g) for (n, g) in test_set]\n",
    "nltk.classify.accuracy(classifier, test_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance on the test set is slightly worse, but given that we have a <em style=\"color:blue;\">bias</em> of 20%, there is no need to worry about a <em style=\"color:blue;\">variance</em> of 2% at this point.  Of course, this remark only holds if we assume that the so called <em style=\"color:blue;\">Bayes optimal error</em> is close to 0%.  If, instead, the Bayes optimal error would be, say, 19%, then we can never achieve an accuracy that is better than 81%. In that case the difference of 2% between the test set and the training set would have to be investigated further, because it is then more promising to reduce this error than to try to reduce the 1% that separates the error on the training set from the best possible error.  Of course, initially we do not know the Bayes optimal error.  For now I am just assuming that it is 15% or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NaiveBayesClassifier` has a useful method called $\\texttt{show_most_informative_features}(n)$ which shows the $n$ most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  ending = 'na'                f : m      =     93.3 : 1.0\n",
      "                  ending = 'la'                f : m      =     69.9 : 1.0\n",
      "                  ending = 'ia'                f : m      =     37.8 : 1.0\n",
      "                  ending = 'sa'                f : m      =     33.8 : 1.0\n",
      "                  ending = 'us'                m : f      =     28.3 : 1.0\n",
      "                  ending = 'io'                m : f      =     26.1 : 1.0\n",
      "                  ending = 'ta'                f : m      =     24.1 : 1.0\n",
      "                  ending = 'ra'                f : m      =     23.9 : 1.0\n",
      "                  ending = 'rt'                m : f      =     23.6 : 1.0\n",
      "                  ending = 'rd'                m : f      =     23.5 : 1.0\n",
      "                  ending = 'ch'                m : f      =     15.0 : 1.0\n",
      "                  ending = 'ea'                f : m      =     13.8 : 1.0\n",
      "                  ending = 'ka'                f : m      =     13.0 : 1.0\n",
      "                  ending = 'em'                m : f      =     12.8 : 1.0\n",
      "                  ending = 'on'                m : f      =     11.0 : 1.0\n",
      "                  ending = 'ns'                m : f      =     10.5 : 1.0\n",
      "                  ending = 'ti'                f : m      =      9.8 : 1.0\n",
      "                  ending = 'im'                m : f      =      9.7 : 1.0\n",
      "                  ending = 'as'                m : f      =      9.5 : 1.0\n",
      "                  ending = 'ud'                m : f      =      9.4 : 1.0\n",
      "                  ending = 'ya'                f : m      =      9.4 : 1.0\n",
      "                  ending = 'di'                f : m      =      8.5 : 1.0\n",
      "                  ending = 'go'                m : f      =      8.3 : 1.0\n",
      "                  ending = 'rk'                m : f      =      8.3 : 1.0\n",
      "                  ending = 'ip'                m : f      =      8.3 : 1.0\n",
      "                  ending = 'ff'                m : f      =      8.3 : 1.0\n",
      "                  ending = 'ul'                m : f      =      8.3 : 1.0\n",
      "                  ending = 'st'                m : f      =      8.3 : 1.0\n",
      "                  ending = 'er'                m : f      =      8.3 : 1.0\n",
      "                  ending = 'am'                m : f      =      7.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this output tell us that for 93 female names ending in `na` there is just one male name that ends in `na`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our goal is to refine our model for gender classification by adding more features.  In order to get a better understanding, let us investigate those names that are misclassified.  We have to be careful to look at examples from the training set, not from the test set, for if we design features with respect to the test set, then the test set will now longer give us a reasonable estimate of the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Scarlett', 'f'),\n",
       " ('Shelby', 'm'),\n",
       " ('Doloritas', 'f'),\n",
       " ('Mendel', 'm'),\n",
       " ('Theo', 'f'),\n",
       " ('Eddy', 'm'),\n",
       " ('Barney', 'm'),\n",
       " ('Uli', 'm'),\n",
       " ('Garret', 'm'),\n",
       " ('Cecil', 'f'),\n",
       " ('Julie', 'm'),\n",
       " ('Hewie', 'm'),\n",
       " ('Dot', 'f'),\n",
       " ('Lonny', 'm'),\n",
       " ('Stanley', 'm'),\n",
       " ('Vinnie', 'm'),\n",
       " ('Haleigh', 'f'),\n",
       " ('Juergen', 'm'),\n",
       " ('Helmuth', 'm'),\n",
       " ('Griffith', 'm'),\n",
       " ('Prince', 'm'),\n",
       " ('Tobie', 'm'),\n",
       " ('Roddie', 'm'),\n",
       " ('Thomasin', 'f'),\n",
       " ('Loren', 'm'),\n",
       " ('Monty', 'm'),\n",
       " ('Archie', 'm'),\n",
       " ('Cherin', 'f'),\n",
       " ('Gene', 'm'),\n",
       " ('Krishna', 'm'),\n",
       " ('Fan', 'f'),\n",
       " ('Chevy', 'm'),\n",
       " ('Sandy', 'm'),\n",
       " ('Ambrosi', 'm'),\n",
       " ('Casey', 'm'),\n",
       " ('Dionis', 'm'),\n",
       " ('Anthony', 'm'),\n",
       " ('Demetris', 'm'),\n",
       " ('Felicdad', 'f'),\n",
       " ('Wayne', 'm'),\n",
       " ('Peg', 'f'),\n",
       " ('Willis', 'm'),\n",
       " ('Devan', 'f'),\n",
       " ('Michael', 'm'),\n",
       " ('Jean', 'f'),\n",
       " ('Dennie', 'm'),\n",
       " ('Mikael', 'm'),\n",
       " ('Barret', 'm'),\n",
       " ('Hervey', 'm'),\n",
       " ('Raphael', 'm'),\n",
       " ('Germain', 'f'),\n",
       " ('Mischa', 'm'),\n",
       " ('Sly', 'm'),\n",
       " ('Taffy', 'f'),\n",
       " ('Giovanni', 'm'),\n",
       " ('Broddy', 'm'),\n",
       " ('Danny', 'm'),\n",
       " ('Keith', 'm'),\n",
       " ('Sunny', 'm'),\n",
       " ('Stacy', 'm'),\n",
       " ('Mose', 'm'),\n",
       " ('Patrice', 'm'),\n",
       " ('Lindy', 'm'),\n",
       " ('Jerry', 'm'),\n",
       " ('Megan', 'f'),\n",
       " ('Wylie', 'm'),\n",
       " ('Waine', 'm'),\n",
       " ('Harvey', 'm'),\n",
       " ('Sybil', 'f'),\n",
       " ('Eleanor', 'f'),\n",
       " ('Vicky', 'f'),\n",
       " ('Zolly', 'm'),\n",
       " ('Gertrud', 'f'),\n",
       " ('Pattie', 'm'),\n",
       " ('Kip', 'f'),\n",
       " ('Llewellyn', 'm'),\n",
       " ('Nickey', 'm'),\n",
       " ('Yance', 'm'),\n",
       " ('Aubrey', 'm'),\n",
       " ('Dagmar', 'f'),\n",
       " ('Lemmy', 'm'),\n",
       " ('Georgie', 'm'),\n",
       " ('Ferdy', 'm'),\n",
       " ('Zoe', 'f'),\n",
       " ('Ambrose', 'm'),\n",
       " ('Calhoun', 'm'),\n",
       " ('Slade', 'm'),\n",
       " ('Wendel', 'm'),\n",
       " ('Gaynor', 'f'),\n",
       " ('Ritchie', 'm'),\n",
       " ('Iggie', 'm'),\n",
       " ('Sully', 'm'),\n",
       " ('Lindsey', 'm'),\n",
       " ('Davie', 'm'),\n",
       " ('Jean-Francois', 'm'),\n",
       " ('Keene', 'm'),\n",
       " ('Hale', 'm'),\n",
       " ('Barty', 'm'),\n",
       " ('Cam', 'f'),\n",
       " ('Willie', 'm'),\n",
       " ('Isaiah', 'm'),\n",
       " ('Herbie', 'm'),\n",
       " ('Sibyl', 'm'),\n",
       " ('Torre', 'm'),\n",
       " ('Rawley', 'm'),\n",
       " ('Dallas', 'f'),\n",
       " ('Jodie', 'm'),\n",
       " ('Bo', 'm'),\n",
       " ('Doyle', 'm'),\n",
       " ('Gerri', 'm'),\n",
       " ('Roni', 'm'),\n",
       " ('Lyn', 'm'),\n",
       " ('Martainn', 'm'),\n",
       " ('Clare', 'm'),\n",
       " ('Val', 'm'),\n",
       " ('Ester', 'f'),\n",
       " ('Christan', 'f'),\n",
       " ('Neel', 'm'),\n",
       " ('Margot', 'f'),\n",
       " ('Perry', 'm'),\n",
       " ('Consuelo', 'f'),\n",
       " ('Lewis', 'm'),\n",
       " ('Miran', 'f'),\n",
       " ('Timmy', 'm'),\n",
       " ('Genvieve', 'f'),\n",
       " ('Courtenay', 'f'),\n",
       " ('Hansel', 'm'),\n",
       " ('Ruben', 'm'),\n",
       " ('Meg', 'f'),\n",
       " ('Vachel', 'm'),\n",
       " ('Rosamond', 'f'),\n",
       " ('Dewey', 'm'),\n",
       " ('Piper', 'f'),\n",
       " ('Hope', 'f'),\n",
       " ('Glen', 'm'),\n",
       " ('Katlin', 'f'),\n",
       " ('Brady', 'm'),\n",
       " ('Allyson', 'f'),\n",
       " ('Ikey', 'm'),\n",
       " ('Hollis', 'm'),\n",
       " ('Clarence', 'm'),\n",
       " ('Easter', 'f'),\n",
       " ('Allen', 'm'),\n",
       " ('Jody', 'm'),\n",
       " ('Duffie', 'm'),\n",
       " ('Rice', 'm'),\n",
       " ('Uriah', 'm'),\n",
       " ('Kerry', 'm'),\n",
       " ('Micky', 'f'),\n",
       " ('Jaquelin', 'f'),\n",
       " ('Giffie', 'm'),\n",
       " ('Parsifal', 'm'),\n",
       " ('Arel', 'm'),\n",
       " ('Noah', 'm'),\n",
       " ('Neville', 'm'),\n",
       " ('Avi', 'm'),\n",
       " ('Bird', 'f'),\n",
       " ('Jordan', 'f'),\n",
       " ('Fanchon', 'f'),\n",
       " ('Piggy', 'm'),\n",
       " ('Luce', 'm'),\n",
       " ('Caroljean', 'f'),\n",
       " ('Maximilien', 'm'),\n",
       " ('Mair', 'f'),\n",
       " ('Rickie', 'm'),\n",
       " ('Penn', 'm'),\n",
       " ('Montague', 'm'),\n",
       " ('Deryl', 'm'),\n",
       " ('Penelope', 'f'),\n",
       " ('Verne', 'm'),\n",
       " ('Chance', 'm'),\n",
       " ('Rustie', 'm'),\n",
       " ('Jeremie', 'm'),\n",
       " ('Meghan', 'f'),\n",
       " ('Johny', 'm'),\n",
       " ('Leanor', 'f'),\n",
       " ('Royce', 'm'),\n",
       " ('Orville', 'm'),\n",
       " ('Kerstin', 'f'),\n",
       " ('Jabez', 'm'),\n",
       " ('Dane', 'm'),\n",
       " ('Cris', 'm'),\n",
       " ('Avery', 'm'),\n",
       " ('Jesse', 'm'),\n",
       " ('Barth', 'm'),\n",
       " ('Shane', 'm'),\n",
       " ('Hedwig', 'f'),\n",
       " ('Hercule', 'm'),\n",
       " ('Augustine', 'm'),\n",
       " ('Sloane', 'm'),\n",
       " ('Johann', 'm'),\n",
       " ('Timmie', 'm'),\n",
       " ('Caro', 'f'),\n",
       " ('Henrique', 'm'),\n",
       " ('Baillie', 'm'),\n",
       " ('Lauren', 'm'),\n",
       " ('Evangelin', 'f'),\n",
       " ('Yancey', 'm'),\n",
       " ('Towney', 'm'),\n",
       " ('Allah', 'm'),\n",
       " ('Sinclare', 'm'),\n",
       " ('Godfrey', 'm'),\n",
       " ('Johnnie', 'm'),\n",
       " ('Angie', 'm'),\n",
       " ('Ashby', 'm'),\n",
       " ('Pen', 'm'),\n",
       " ('Eugen', 'm'),\n",
       " ('Jeth', 'm'),\n",
       " ('Karil', 'f'),\n",
       " ('Elden', 'm'),\n",
       " ('Burl', 'm'),\n",
       " ('Hermy', 'm'),\n",
       " ('Clayborne', 'm'),\n",
       " ('Winn', 'm'),\n",
       " ('Warren', 'm'),\n",
       " ('Torrey', 'm'),\n",
       " ('Emmanuel', 'm'),\n",
       " ('Klee', 'm'),\n",
       " ('Clair', 'f'),\n",
       " ('Jackie', 'm'),\n",
       " ('Dwayne', 'm'),\n",
       " ('Elijah', 'm'),\n",
       " ('Adelind', 'f'),\n",
       " ('Brittan', 'f'),\n",
       " ('Wynn', 'm'),\n",
       " ('Kalle', 'm'),\n",
       " ('Mel', 'm'),\n",
       " ('Haley', 'm'),\n",
       " ('Maurise', 'm'),\n",
       " ('Guthrie', 'm'),\n",
       " ('Catlin', 'f'),\n",
       " ('Earle', 'm'),\n",
       " ('Samuele', 'm'),\n",
       " ('Ike', 'f'),\n",
       " ('Ben', 'm'),\n",
       " ('Alley', 'm'),\n",
       " ('Zechariah', 'm'),\n",
       " ('Weslie', 'm'),\n",
       " ('Suzan', 'f'),\n",
       " ('Curtice', 'm'),\n",
       " ('Melvyn', 'm'),\n",
       " ('Barris', 'm'),\n",
       " ('Hezekiah', 'm'),\n",
       " ('Smith', 'm'),\n",
       " ('Avril', 'f'),\n",
       " ('Rodney', 'm'),\n",
       " ('Tarrance', 'm'),\n",
       " ('Dickie', 'm'),\n",
       " ('Voltaire', 'm'),\n",
       " ('Pooh', 'm'),\n",
       " ('Edie', 'm'),\n",
       " ('Brody', 'm'),\n",
       " ('Carsten', 'm'),\n",
       " ('Arvy', 'm'),\n",
       " ('Cody', 'm'),\n",
       " ('Douglis', 'm'),\n",
       " ('Emmit', 'm'),\n",
       " ('Aguste', 'm'),\n",
       " ('Fonzie', 'm'),\n",
       " ('Jeffry', 'm'),\n",
       " ('Janot', 'f'),\n",
       " ('Amery', 'm'),\n",
       " ('Warden', 'm'),\n",
       " ('Blayne', 'm'),\n",
       " ('Alfonse', 'm'),\n",
       " ('Caitlin', 'f'),\n",
       " ('Ricki', 'm'),\n",
       " ('Garvey', 'm'),\n",
       " ('Blake', 'f'),\n",
       " ('Neddie', 'm'),\n",
       " ('Bertie', 'm'),\n",
       " ('Wallace', 'm'),\n",
       " ('Davy', 'm'),\n",
       " ('Anett', 'f'),\n",
       " ('Bret', 'm'),\n",
       " ('Vasili', 'm'),\n",
       " ('Len', 'm'),\n",
       " ('Archy', 'm'),\n",
       " ('Clarance', 'm'),\n",
       " ('Charmain', 'f'),\n",
       " ('Gabriel', 'm'),\n",
       " ('Walden', 'm'),\n",
       " ('Georgy', 'm'),\n",
       " ('Rabi', 'm'),\n",
       " ('Corby', 'm'),\n",
       " ('Clyde', 'm'),\n",
       " ('Morris', 'm'),\n",
       " ('Hamel', 'm'),\n",
       " ('Yancy', 'm'),\n",
       " ('Nevile', 'm'),\n",
       " ('Tabby', 'm'),\n",
       " ('Marven', 'm'),\n",
       " ('Huey', 'm'),\n",
       " ('Frances', 'f'),\n",
       " ('Hershel', 'm'),\n",
       " ('Davidde', 'm'),\n",
       " ('Jeromy', 'm'),\n",
       " ('Aldis', 'm'),\n",
       " ('Josiah', 'm'),\n",
       " ('Gilligan', 'f'),\n",
       " ('Verney', 'm'),\n",
       " ('Alix', 'm'),\n",
       " ('Riannon', 'f'),\n",
       " ('Tremayne', 'm'),\n",
       " ('Carolan', 'f'),\n",
       " ('Sebastien', 'm'),\n",
       " ('Harvie', 'm'),\n",
       " ('Ezechiel', 'm'),\n",
       " ('Worthy', 'm'),\n",
       " ('Jermaine', 'm'),\n",
       " ('Jazmin', 'f'),\n",
       " ('Jeramie', 'm'),\n",
       " ('Henry', 'm'),\n",
       " ('Holly', 'm'),\n",
       " ('Michal', 'm'),\n",
       " ('Herb', 'm'),\n",
       " ('Abdel', 'm'),\n",
       " ('Rusty', 'm'),\n",
       " ('Shell', 'f'),\n",
       " ('Matty', 'm'),\n",
       " ('Sammie', 'm'),\n",
       " ('Scotty', 'm'),\n",
       " ('Townie', 'm'),\n",
       " ('Hymie', 'm'),\n",
       " ('Orren', 'm'),\n",
       " ('Duane', 'm'),\n",
       " ('Anatoly', 'm'),\n",
       " ('Lilias', 'f'),\n",
       " ('Nickie', 'm'),\n",
       " ('Marietta', 'm'),\n",
       " ('Travis', 'm'),\n",
       " ('Patsy', 'm'),\n",
       " ('Neal', 'm'),\n",
       " ('Madlin', 'f'),\n",
       " ('Davide', 'm'),\n",
       " ('Emmy', 'm'),\n",
       " ('Jorge', 'm'),\n",
       " ('Charlot', 'f'),\n",
       " ('Torrence', 'm'),\n",
       " ('Dominique', 'm'),\n",
       " ('Hildegaard', 'f'),\n",
       " ('Godfree', 'm'),\n",
       " ('Jennifer', 'f'),\n",
       " ('Spence', 'm'),\n",
       " ('Muffin', 'f'),\n",
       " ('Barnaby', 'm'),\n",
       " ('Patel', 'm'),\n",
       " ('Maurie', 'm'),\n",
       " ('Eugene', 'm'),\n",
       " ('Stevy', 'm'),\n",
       " ('Wolfie', 'm'),\n",
       " ('Tulley', 'm'),\n",
       " ('Jamie', 'm'),\n",
       " ('Brice', 'm'),\n",
       " ('Robbie', 'm'),\n",
       " ('Herculie', 'm'),\n",
       " ('Kaylil', 'f'),\n",
       " ('Wadsworth', 'm'),\n",
       " ('Lind', 'f'),\n",
       " ('Sidnee', 'm'),\n",
       " ('Immanuel', 'm'),\n",
       " ('Perceval', 'm'),\n",
       " ('Tammie', 'm'),\n",
       " ('Elmore', 'm'),\n",
       " ('Judy', 'm'),\n",
       " ('Karsten', 'm'),\n",
       " ('Elvis', 'm'),\n",
       " ('Arvie', 'm'),\n",
       " ('Hamnet', 'm'),\n",
       " ('Wallis', 'm'),\n",
       " ('Reilly', 'm'),\n",
       " ('Gordie', 'm'),\n",
       " ('Juanita', 'm'),\n",
       " ('Isa', 'm'),\n",
       " ('Gavriel', 'm'),\n",
       " ('Jarvis', 'm'),\n",
       " ('Angel', 'm'),\n",
       " ('Aimil', 'f'),\n",
       " ('Lemmie', 'm'),\n",
       " ('Gregory', 'm'),\n",
       " ('Ajay', 'f'),\n",
       " ('Finley', 'm'),\n",
       " ('Joscelin', 'f'),\n",
       " ('Vance', 'm'),\n",
       " ('Allyn', 'm'),\n",
       " ('Pavel', 'm'),\n",
       " ('Benn', 'm'),\n",
       " ('Merrill', 'f'),\n",
       " ('Pennie', 'm'),\n",
       " ('Micheal', 'm'),\n",
       " ('Tony', 'm'),\n",
       " ('Marian', 'f'),\n",
       " ('Kat', 'f'),\n",
       " ('Arne', 'm'),\n",
       " ('Seth', 'm'),\n",
       " ('Jewell', 'f'),\n",
       " ('Barrie', 'm'),\n",
       " ('Haskel', 'm'),\n",
       " ('Jimmie', 'm'),\n",
       " ('Zippy', 'm'),\n",
       " ('Westley', 'm'),\n",
       " ('Evelyn', 'm'),\n",
       " ('Andie', 'm'),\n",
       " ('Yigal', 'm'),\n",
       " ('Louie', 'm'),\n",
       " ('Garcia', 'm'),\n",
       " ('Addie', 'm'),\n",
       " ('Seymour', 'm'),\n",
       " ('Arie', 'm'),\n",
       " ('Lemuel', 'm'),\n",
       " ('Flor', 'f'),\n",
       " ('Bela', 'm'),\n",
       " ('Prasun', 'm'),\n",
       " ('Laurance', 'm'),\n",
       " ('Tamar', 'f'),\n",
       " ('Serge', 'm'),\n",
       " ('Levy', 'm'),\n",
       " ('Myriam', 'f'),\n",
       " ('Earl', 'm'),\n",
       " ('Germaine', 'm'),\n",
       " ('Max', 'f'),\n",
       " ('Sal', 'm'),\n",
       " ('Dion', 'f'),\n",
       " ('Catherin', 'f'),\n",
       " ('Caitrin', 'f'),\n",
       " ('Hillary', 'm'),\n",
       " ('Tommie', 'm'),\n",
       " ('Terrence', 'm'),\n",
       " ('Normie', 'm'),\n",
       " ('Clancy', 'm'),\n",
       " ('Jory', 'm'),\n",
       " ('Morlee', 'm'),\n",
       " ('Costa', 'm'),\n",
       " ('Paddie', 'm'),\n",
       " ('Wit', 'm'),\n",
       " ('Robinett', 'f'),\n",
       " ('Spense', 'm'),\n",
       " ('Denny', 'm'),\n",
       " ('Darth', 'm'),\n",
       " ('Larry', 'm'),\n",
       " ('Cleo', 'f'),\n",
       " ('Sher', 'f'),\n",
       " ('Chicky', 'f'),\n",
       " ('Dmitri', 'm'),\n",
       " ('Lian', 'f'),\n",
       " ('Cristin', 'f'),\n",
       " ('Bailey', 'm'),\n",
       " ('Rene', 'm'),\n",
       " ('Merle', 'm'),\n",
       " ('Case', 'm'),\n",
       " ('Kermit', 'm'),\n",
       " ('Cole', 'm'),\n",
       " ('Cher', 'f'),\n",
       " ('Lorain', 'f'),\n",
       " ('Anatole', 'm'),\n",
       " ('Jaynell', 'f'),\n",
       " ('Rozamond', 'f'),\n",
       " ('Gert', 'f'),\n",
       " ('Andy', 'm'),\n",
       " ('Ishmael', 'm'),\n",
       " ('Chrissy', 'm'),\n",
       " ('Mortie', 'm'),\n",
       " ('Dudley', 'm'),\n",
       " ('Meryl', 'm'),\n",
       " ('Stillmann', 'm'),\n",
       " ('Kaitlin', 'f'),\n",
       " ('Mellisent', 'f'),\n",
       " ('Andrey', 'm'),\n",
       " ('Kerrin', 'f'),\n",
       " ('Zackariah', 'm'),\n",
       " ('Hilary', 'm'),\n",
       " ('Nichole', 'm'),\n",
       " ('Harriot', 'f'),\n",
       " ('Parry', 'm'),\n",
       " ('Mame', 'f'),\n",
       " ('Lynnett', 'f'),\n",
       " ('Lane', 'm'),\n",
       " ('Pate', 'm'),\n",
       " ('Grier', 'f'),\n",
       " ('Michel', 'm'),\n",
       " ('Kenn', 'm'),\n",
       " ('Emmet', 'm'),\n",
       " ('Blanch', 'f'),\n",
       " ('Willow', 'f'),\n",
       " ('Oren', 'm'),\n",
       " ('Danell', 'f'),\n",
       " ('Aime', 'f'),\n",
       " ('Brier', 'f'),\n",
       " ('Glynn', 'm'),\n",
       " ('Clemente', 'm'),\n",
       " ('Joannes', 'f'),\n",
       " ('Gray', 'f'),\n",
       " ('Vasilis', 'm'),\n",
       " ('Harriett', 'f'),\n",
       " ('Danie', 'm'),\n",
       " ('Russel', 'm'),\n",
       " ('Abbie', 'm'),\n",
       " ('Tommy', 'm'),\n",
       " ('Giavani', 'm'),\n",
       " ('Regan', 'f'),\n",
       " ('Raf', 'f'),\n",
       " ('Moise', 'm'),\n",
       " ('Torry', 'm'),\n",
       " ('Mustafa', 'm'),\n",
       " ('Leslie', 'm'),\n",
       " ('Joyous', 'f'),\n",
       " ('Brear', 'f'),\n",
       " ('Niven', 'm'),\n",
       " ('Barny', 'm'),\n",
       " ('Dory', 'm'),\n",
       " ('Devin', 'f'),\n",
       " ('Darien', 'm'),\n",
       " ('Jimmy', 'm'),\n",
       " ('Jerzy', 'm'),\n",
       " ('Uriel', 'm'),\n",
       " ('Bary', 'm'),\n",
       " ('Tyrone', 'm'),\n",
       " ('Emmery', 'm'),\n",
       " ('Brandy', 'm'),\n",
       " ('Verge', 'm'),\n",
       " ('Gayle', 'm'),\n",
       " ('Clinten', 'm'),\n",
       " ('Ulrike', 'f'),\n",
       " ('Delores', 'f'),\n",
       " ('Jan', 'f'),\n",
       " ('Glenn', 'm'),\n",
       " ('Abdullah', 'm'),\n",
       " ('Rafael', 'm'),\n",
       " ('Janifer', 'f'),\n",
       " ('Kelley', 'm'),\n",
       " ('Izzy', 'm'),\n",
       " ('Morten', 'm'),\n",
       " ('Nicky', 'f'),\n",
       " ('Drew', 'f'),\n",
       " ('Elinor', 'f'),\n",
       " ('Lil', 'f'),\n",
       " ('Sammy', 'm'),\n",
       " ('Beilul', 'f'),\n",
       " ('Sterne', 'm'),\n",
       " ('Blaine', 'm'),\n",
       " ('Kelly', 'm'),\n",
       " ('Winnifred', 'f'),\n",
       " ('Aditya', 'm'),\n",
       " ('Worth', 'm'),\n",
       " ('Blair', 'f'),\n",
       " ('Harris', 'm'),\n",
       " ('Robin', 'f'),\n",
       " ('Cortese', 'm'),\n",
       " ('Beau', 'm'),\n",
       " ('Wilden', 'm'),\n",
       " ('Orbadiah', 'm'),\n",
       " ('Milissent', 'f'),\n",
       " ('Joey', 'm'),\n",
       " ('Bev', 'f'),\n",
       " ('Marcel', 'm'),\n",
       " ('Kim', 'f'),\n",
       " ('Melisent', 'f'),\n",
       " ('Maury', 'm'),\n",
       " ('Boris', 'm'),\n",
       " ('Ray', 'f'),\n",
       " ('Francis', 'm'),\n",
       " ('Raynell', 'f'),\n",
       " ('Amber', 'f'),\n",
       " ('Tuesday', 'f'),\n",
       " ('Lucky', 'f'),\n",
       " ('Hildagard', 'f'),\n",
       " ('Cobbie', 'm'),\n",
       " ('Reggy', 'm'),\n",
       " ('Granville', 'm'),\n",
       " ('Dabney', 'm'),\n",
       " ('Flo', 'f'),\n",
       " ('Ted', 'f'),\n",
       " ('Virgie', 'm'),\n",
       " ('Dougie', 'm'),\n",
       " ('Giovanne', 'm'),\n",
       " ('Lust', 'f'),\n",
       " ('Tallie', 'm'),\n",
       " ('Rutledge', 'm'),\n",
       " ('Dimitri', 'm'),\n",
       " ('Ealasaid', 'f'),\n",
       " ('See', 'm'),\n",
       " ('Ivett', 'f'),\n",
       " ('Gus', 'f'),\n",
       " ('Gabe', 'm'),\n",
       " ('Lenny', 'm'),\n",
       " ('Teddie', 'm'),\n",
       " ('Lilyan', 'f'),\n",
       " ('Fidel', 'm'),\n",
       " ('Rabbi', 'm'),\n",
       " ('Ruddie', 'm'),\n",
       " ('Sturgis', 'm'),\n",
       " ('Salome', 'f'),\n",
       " ('Zollie', 'm'),\n",
       " ('Lennie', 'm'),\n",
       " ('Courtney', 'm'),\n",
       " ('Grace', 'm'),\n",
       " ('Sergei', 'm'),\n",
       " ('Daffy', 'f'),\n",
       " ('Jeremy', 'm'),\n",
       " ('Curtis', 'm'),\n",
       " ('Felice', 'm'),\n",
       " ('Aziz', 'm'),\n",
       " ('Sheffie', 'm'),\n",
       " ('Jacquelin', 'f'),\n",
       " ('Donn', 'm'),\n",
       " ('Errol', 'm'),\n",
       " ('Morrie', 'm'),\n",
       " ('Isidore', 'm'),\n",
       " ('Noel', 'm'),\n",
       " ('Terrel', 'm'),\n",
       " ('Madelin', 'f'),\n",
       " ('Ansel', 'm'),\n",
       " ('Trey', 'm'),\n",
       " ('Garland', 'f'),\n",
       " ('Vernen', 'm'),\n",
       " ('Ronnie', 'm'),\n",
       " ('Tracey', 'm'),\n",
       " ('Millicent', 'f'),\n",
       " ('Del', 'm'),\n",
       " ('Honor', 'f'),\n",
       " ('Godfry', 'm'),\n",
       " ('Virge', 'm'),\n",
       " ('Russ', 'm'),\n",
       " ('Nanon', 'f'),\n",
       " ('Tannie', 'm'),\n",
       " ('Jillian', 'f'),\n",
       " ('Gennifer', 'f'),\n",
       " ('Alphonse', 'm'),\n",
       " ('Dell', 'f'),\n",
       " ('Rosario', 'f'),\n",
       " ('Berkley', 'm'),\n",
       " ('Winnie', 'm'),\n",
       " ('Samuel', 'm'),\n",
       " ('Tiffy', 'f'),\n",
       " ('Caron', 'f'),\n",
       " ('Caryl', 'm'),\n",
       " ('Kai', 'f'),\n",
       " ('Terry', 'm'),\n",
       " ('Hermann', 'm'),\n",
       " ('Flower', 'f'),\n",
       " ('Berkeley', 'm'),\n",
       " ('Jonny', 'm'),\n",
       " ('Christean', 'f'),\n",
       " ('Linell', 'f'),\n",
       " ('Ruddy', 'm'),\n",
       " ('Mackenzie', 'm'),\n",
       " ('Ari', 'm'),\n",
       " ('Zachary', 'm'),\n",
       " ('Moshe', 'm'),\n",
       " ('Woodie', 'm'),\n",
       " ('Chen', 'm'),\n",
       " ('Alex', 'f'),\n",
       " ('Quiggly', 'm'),\n",
       " ('Nealy', 'm'),\n",
       " ('Luigi', 'm'),\n",
       " ('Demosthenis', 'm'),\n",
       " ('Hamlet', 'm'),\n",
       " ('Shumeet', 'm'),\n",
       " ('Aristotle', 'm'),\n",
       " ('Jephthah', 'm'),\n",
       " ('Richy', 'm'),\n",
       " ('Meaghan', 'f'),\n",
       " ('Tully', 'm'),\n",
       " ('Dante', 'm'),\n",
       " ('Carlyle', 'm'),\n",
       " ('Partha', 'm'),\n",
       " ('Flinn', 'm'),\n",
       " ('Jackquelin', 'f'),\n",
       " ('Mercedes', 'f'),\n",
       " ('Thane', 'm'),\n",
       " ('Nate', 'm'),\n",
       " ('Roch', 'f'),\n",
       " ('Stanly', 'm'),\n",
       " ('Marillin', 'f'),\n",
       " ('Mag', 'f'),\n",
       " ('Mildred', 'f'),\n",
       " ('Roselin', 'f'),\n",
       " ('Lefty', 'm'),\n",
       " ('Brooke', 'f'),\n",
       " ('Eve', 'f'),\n",
       " ('Vin', 'f'),\n",
       " ('Judith', 'm'),\n",
       " ('Gabriele', 'm'),\n",
       " ('Shaun', 'm'),\n",
       " ('Lyndsay', 'f'),\n",
       " ('Love', 'f'),\n",
       " ('Jess', 'm'),\n",
       " ('Donnie', 'm'),\n",
       " ('Freddy', 'm'),\n",
       " ('Vite', 'm'),\n",
       " ('Darryl', 'm'),\n",
       " ('Charmian', 'f'),\n",
       " ('Dew', 'f'),\n",
       " ('Olive', 'f'),\n",
       " ('Bubba', 'm'),\n",
       " ('Petey', 'm'),\n",
       " ('Prentiss', 'm'),\n",
       " ('Becky', 'f'),\n",
       " ('Ezekiel', 'm'),\n",
       " ('Jefferey', 'm'),\n",
       " ('Christie', 'm'),\n",
       " ('Nathanial', 'm'),\n",
       " ('Jule', 'm'),\n",
       " ('Maurice', 'm'),\n",
       " ('Constantine', 'm'),\n",
       " ('Tedie', 'm'),\n",
       " ('Gail', 'f'),\n",
       " ('Christy', 'm'),\n",
       " ('Hayden', 'm'),\n",
       " ('Ashleigh', 'f'),\n",
       " ('Tracie', 'm'),\n",
       " ('Lorne', 'm'),\n",
       " ('Odie', 'm'),\n",
       " ('Sinead', 'f'),\n",
       " ('Irvine', 'm'),\n",
       " ('Salomone', 'm'),\n",
       " ('Abbey', 'm'),\n",
       " ('Nikita', 'm'),\n",
       " ('Alison', 'f'),\n",
       " ('Vivyan', 'f'),\n",
       " ('Eben', 'm'),\n",
       " ('Kennedy', 'm'),\n",
       " ('Theodore', 'm'),\n",
       " ('Teddy', 'm'),\n",
       " ('Mendie', 'm'),\n",
       " ('Dorian', 'f'),\n",
       " ('Charlie', 'm'),\n",
       " ('Jacky', 'f'),\n",
       " ('Toby', 'm'),\n",
       " ('Hodge', 'm'),\n",
       " ('Elke', 'f'),\n",
       " ('Noble', 'm'),\n",
       " ('Antoni', 'm'),\n",
       " ('Skelly', 'm'),\n",
       " ('Hoyt', 'm'),\n",
       " ('Owen', 'm'),\n",
       " ('Shelley', 'm'),\n",
       " ('Reube', 'm'),\n",
       " ('Michell', 'f'),\n",
       " ('Alexis', 'm'),\n",
       " ('Mikel', 'm'),\n",
       " ('Wiley', 'm'),\n",
       " ('Skippie', 'm'),\n",
       " ('Natale', 'm'),\n",
       " ('Thaine', 'm'),\n",
       " ('Jamey', 'm'),\n",
       " ('Clo', 'f'),\n",
       " ('Ginger', 'f'),\n",
       " ('Henri', 'm'),\n",
       " ('Bealle', 'm'),\n",
       " ('Darcy', 'm'),\n",
       " ('Jose', 'm'),\n",
       " ('Sonny', 'm'),\n",
       " ('Nelsen', 'm'),\n",
       " ('Francois', 'm'),\n",
       " ('Goose', 'm'),\n",
       " ('Ken', 'm'),\n",
       " ('Tuckie', 'm'),\n",
       " ('Trish', 'f'),\n",
       " ('Gay', 'f'),\n",
       " ('Donny', 'm'),\n",
       " ('Sileas', 'f'),\n",
       " ('Kaster', 'f'),\n",
       " ('Rhiamon', 'f'),\n",
       " ('Rozalin', 'f'),\n",
       " ('Baily', 'm'),\n",
       " ('Morly', 'm'),\n",
       " ('Donnajean', 'f'),\n",
       " ('Lonnie', 'm'),\n",
       " ('Lyndell', 'f'),\n",
       " ('Kirstin', 'f'),\n",
       " ('Eddie', 'm'),\n",
       " ('Nat', 'f'),\n",
       " ('Derrol', 'm'),\n",
       " ('Jourdan', 'f'),\n",
       " ('Greggory', 'm'),\n",
       " ('Hamlen', 'm'),\n",
       " ('Donal', 'm'),\n",
       " ('Wye', 'm'),\n",
       " ('Zedekiah', 'm'),\n",
       " ('Leigh', 'f'),\n",
       " ('Yale', 'm'),\n",
       " ('Meade', 'm'),\n",
       " ('Farrand', 'f'),\n",
       " ('Micah', 'm'),\n",
       " ('Marlo', 'f'),\n",
       " ('Neddy', 'm'),\n",
       " ('Zebadiah', 'm'),\n",
       " ('Bell', 'f'),\n",
       " ('Miguel', 'm'),\n",
       " ('Manny', 'm'),\n",
       " ('Reggis', 'm'),\n",
       " ('Cat', 'f'),\n",
       " ('Gerry', 'm'),\n",
       " ('Carmon', 'f'),\n",
       " ('Row', 'f'),\n",
       " ('Claybourne', 'm'),\n",
       " ('Finn', 'm'),\n",
       " ('Greer', 'f'),\n",
       " ('Buffy', 'f'),\n",
       " ('Worden', 'm'),\n",
       " ('Kyle', 'm'),\n",
       " ('Aidan', 'f'),\n",
       " ('Bobby', 'm'),\n",
       " ('Mickey', 'm'),\n",
       " ('Ole', 'm'),\n",
       " ('Gunvor', 'f'),\n",
       " ('Ashley', 'm'),\n",
       " ('Lyle', 'm'),\n",
       " ('Benjie', 'm'),\n",
       " ('Chloe', 'f'),\n",
       " ('Torey', 'm'),\n",
       " ('Chaddy', 'm'),\n",
       " ('Axel', 'm'),\n",
       " ('Kris', 'm'),\n",
       " ('Mace', 'm'),\n",
       " ('Trace', 'm'),\n",
       " ('Phil', 'f'),\n",
       " ('Norm', 'm'),\n",
       " ('Lynn', 'm'),\n",
       " ('Barthel', 'm'),\n",
       " ('Lucien', 'm'),\n",
       " ('Brook', 'f'),\n",
       " ('Millisent', 'f'),\n",
       " ('Pierce', 'm'),\n",
       " ('Louis', 'm'),\n",
       " ('Morry', 'm'),\n",
       " ('Bjorne', 'm'),\n",
       " ('Mickie', 'm'),\n",
       " ('Tobiah', 'm'),\n",
       " ('Jeremiah', 'm'),\n",
       " ('Zebedee', 'm'),\n",
       " ('Yank', 'm'),\n",
       " ('Durante', 'm'),\n",
       " ('Osbourne', 'm'),\n",
       " ('Jaclin', 'f'),\n",
       " ('Rodge', 'm'),\n",
       " ('Morgan', 'f'),\n",
       " ('Roddy', 'm'),\n",
       " ('Rudy', 'm'),\n",
       " ('Skye', 'm'),\n",
       " ('April', 'f'),\n",
       " ('Eustace', 'm'),\n",
       " ('Miriam', 'f'),\n",
       " ('Ethelin', 'f'),\n",
       " ('Franklyn', 'm'),\n",
       " ('Yardley', 'm'),\n",
       " ('Benjamen', 'm'),\n",
       " ('Montgomery', 'm'),\n",
       " ('Quincey', 'm'),\n",
       " ('Ignace', 'm'),\n",
       " ('Jermayne', 'm'),\n",
       " ('Tammy', 'm'),\n",
       " ('Geoffry', 'm'),\n",
       " ('Brodie', 'm'),\n",
       " ('Ines', 'f'),\n",
       " ('Doralin', 'f'),\n",
       " ('Rodrique', 'm'),\n",
       " ('Cornellis', 'm'),\n",
       " ('Ingrid', 'f'),\n",
       " ('Charlott', 'f'),\n",
       " ('Billy', 'm'),\n",
       " ('Carol-Jean', 'f'),\n",
       " ('Hazel', 'm'),\n",
       " ('Fred', 'f'),\n",
       " ('Terri', 'm'),\n",
       " ('Reza', 'm'),\n",
       " ('Kristin', 'f'),\n",
       " ('Tim', 'f'),\n",
       " ('Laurence', 'm'),\n",
       " ('Karl', 'm'),\n",
       " ('Chet', 'm'),\n",
       " ('Sansone', 'm'),\n",
       " ('Bradley', 'm'),\n",
       " ('Nell', 'f'),\n",
       " ('Emanuel', 'm'),\n",
       " ('Kerrill', 'f'),\n",
       " ('Georgia', 'm'),\n",
       " ('Rosalind', 'f'),\n",
       " ('Cyrille', 'm'),\n",
       " ('Siddhartha', 'm'),\n",
       " ('Aeriell', 'f'),\n",
       " ('Clemmie', 'm'),\n",
       " ('Bruce', 'm'),\n",
       " ('Dolores', 'f'),\n",
       " ('Che', 'm'),\n",
       " ('Peirce', 'm'),\n",
       " ('Paten', 'm'),\n",
       " ('Cory', 'm'),\n",
       " ('Venus', 'f'),\n",
       " ('Diamond', 'f'),\n",
       " ('Gillian', 'f'),\n",
       " ('Davey', 'm'),\n",
       " ('Tait', 'm'),\n",
       " ('Manuel', 'm'),\n",
       " ('Bartel', 'm'),\n",
       " ('Winny', 'm'),\n",
       " ('Doro', 'f'),\n",
       " ('Ellsworth', 'm'),\n",
       " ('Pace', 'm'),\n",
       " ('Mairead', 'f'),\n",
       " ('Franky', 'f'),\n",
       " ('Hasty', 'm'),\n",
       " ('Karylin', 'f'),\n",
       " ('Noell', 'f'),\n",
       " ('Howie', 'm'),\n",
       " ('Dian', 'f'),\n",
       " ('Jayme', 'f'),\n",
       " ('Hillel', 'm'),\n",
       " ('Min', 'f'),\n",
       " ('Woody', 'm'),\n",
       " ('Karon', 'f'),\n",
       " ('Thorny', 'm'),\n",
       " ('Joel', 'm'),\n",
       " ('Clem', 'f'),\n",
       " ('Nike', 'f'),\n",
       " ('Tore', 'm'),\n",
       " ('Gillan', 'f'),\n",
       " ('Isadore', 'm'),\n",
       " ('Cameo', 'f'),\n",
       " ('Fazeel', 'm'),\n",
       " ('Andonis', 'm'),\n",
       " ('Scotti', 'm'),\n",
       " ('Tremaine', 'm'),\n",
       " ('Chanderjit', 'm'),\n",
       " ('Morse', 'm'),\n",
       " ('Horace', 'm'),\n",
       " ('Erl', 'm'),\n",
       " ('Chase', 'm'),\n",
       " ('Brooks', 'f'),\n",
       " ('Torrance', 'm'),\n",
       " ('Wilow', 'f'),\n",
       " ('Robbin', 'f'),\n",
       " ('Karel', 'm'),\n",
       " ('Simone', 'm'),\n",
       " ('Lanny', 'm'),\n",
       " ('Haven', 'm'),\n",
       " ('Tiff', 'f'),\n",
       " ('Karin', 'f'),\n",
       " ('Kit', 'm'),\n",
       " ('Starr', 'f'),\n",
       " ('Shannon', 'f'),\n",
       " ('Charin', 'f'),\n",
       " ('Otis', 'm'),\n",
       " ('Rayshell', 'f'),\n",
       " ('Dorcas', 'f'),\n",
       " ('Madelon', 'f'),\n",
       " ('Merrel', 'm'),\n",
       " ('Jessee', 'm'),\n",
       " ('Chrysler', 'f'),\n",
       " ('Adriaens', 'f'),\n",
       " ('Sherill', 'f'),\n",
       " ('Pasquale', 'm'),\n",
       " ('Chaddie', 'm'),\n",
       " ('Georgiamay', 'f'),\n",
       " ('Judah', 'm'),\n",
       " ('Philippe', 'f'),\n",
       " ('Reggie', 'm'),\n",
       " ('Daryl', 'm'),\n",
       " ('Lark', 'f'),\n",
       " ('Warde', 'm'),\n",
       " ('Fay', 'f'),\n",
       " ('Hartley', 'm'),\n",
       " ('Witty', 'm'),\n",
       " ('Agnes', 'f'),\n",
       " ('Chancey', 'm'),\n",
       " ('Glad', 'f'),\n",
       " ('Joy', 'f'),\n",
       " ('Pascale', 'm'),\n",
       " ('Siobhan', 'f'),\n",
       " ('Dru', 'm'),\n",
       " ('Abigail', 'f'),\n",
       " ('Lazare', 'm'),\n",
       " ('Jessey', 'm'),\n",
       " ('Obie', 'm'),\n",
       " ('Maxie', 'm'),\n",
       " ('Zacharia', 'm'),\n",
       " ('Jerri', 'm'),\n",
       " ('Joyan', 'f'),\n",
       " ('Esau', 'm'),\n",
       " ('Maddy', 'm'),\n",
       " ('Alden', 'm'),\n",
       " ('Carmine', 'm'),\n",
       " ('Mildrid', 'f'),\n",
       " ('Wesley', 'm'),\n",
       " ('Yuri', 'm'),\n",
       " ('Zacharie', 'm'),\n",
       " ('Lorrie', 'm'),\n",
       " ('Bennie', 'm'),\n",
       " ('Dannie', 'm'),\n",
       " ('Caril', 'f'),\n",
       " ('Israel', 'm'),\n",
       " ('Sasha', 'm'),\n",
       " ('Timothee', 'm'),\n",
       " ('Paddy', 'm'),\n",
       " ('Carroll', 'f'),\n",
       " ('Manon', 'f'),\n",
       " ('Sydney', 'm'),\n",
       " ('Waite', 'm'),\n",
       " ('Kirby', 'm'),\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [(n, g) for (n, g) in train_set \n",
    "                 if classifier.classify(gender_features(n)) != g\n",
    "         ]\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first attempt to improve our model is to add the first letter that occur in a given word.  Furthermore, we check the letters that occur in a name.  Below is the new definition of the function `gender_features` that has these new features.  We import the module `string` because it provides the function `lower` that converts a string into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def gender_features(name):\n",
    "    features = {}\n",
    "    features[\"first\" ] = name[0].lower()\n",
    "    features[\"suffix\"] = name[-2:].lower()\n",
    "    for letter in string.ascii_lowercase:\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on our old friend `'Hugo'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 'h',\n",
       " 'suffix': 'go',\n",
       " 'has(a)': False,\n",
       " 'has(b)': False,\n",
       " 'has(c)': False,\n",
       " 'has(d)': False,\n",
       " 'has(e)': False,\n",
       " 'has(f)': False,\n",
       " 'has(g)': True,\n",
       " 'has(h)': True,\n",
       " 'has(i)': False,\n",
       " 'has(j)': False,\n",
       " 'has(k)': False,\n",
       " 'has(l)': False,\n",
       " 'has(m)': False,\n",
       " 'has(n)': False,\n",
       " 'has(o)': True,\n",
       " 'has(p)': False,\n",
       " 'has(q)': False,\n",
       " 'has(r)': False,\n",
       " 'has(s)': False,\n",
       " 'has(t)': False,\n",
       " 'has(u)': True,\n",
       " 'has(v)': False,\n",
       " 'has(w)': False,\n",
       " 'has(x)': False,\n",
       " 'has(y)': False,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('Hugo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gender_features('Hugo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new implementation of the function `gender_features` we have 28 features, which is a lot more than what we had in our first model.  But do these additional features actually improve the performance of our model?  We can only answer this question if we train the model and test it.  Let us compute the features of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_features = [(gender_features(n), g) for (n, g) in train_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a `NaiveBayesClassifier` with the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check whether the accuracy for the training set has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8131428571428572"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, train_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8040254237288136"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_features = [(gender_features(n), g) for (n, g) in test_set]\n",
    "nltk.classify.accuracy(classifier, test_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this is an improvement, but this improvement is less than what we might have hoped for.  Let us check the 30 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  suffix = 'na'                f : m      =     93.2 : 1.0\n",
      "                  suffix = 'la'                f : m      =     70.2 : 1.0\n",
      "                  suffix = 'ia'                f : m      =     37.8 : 1.0\n",
      "                  suffix = 'sa'                f : m      =     33.8 : 1.0\n",
      "                  suffix = 'us'                m : f      =     28.3 : 1.0\n",
      "                  suffix = 'io'                m : f      =     26.1 : 1.0\n",
      "                  suffix = 'ta'                f : m      =     24.1 : 1.0\n",
      "                  suffix = 'ra'                f : m      =     23.9 : 1.0\n",
      "                  suffix = 'rt'                m : f      =     23.6 : 1.0\n",
      "                  suffix = 'rd'                m : f      =     23.5 : 1.0\n",
      "                  suffix = 'ch'                m : f      =     15.0 : 1.0\n",
      "                  suffix = 'ea'                f : m      =     13.8 : 1.0\n",
      "                  suffix = 'ka'                f : m      =     13.0 : 1.0\n",
      "                  suffix = 'on'                m : f      =     11.1 : 1.0\n",
      "                  suffix = 'ns'                m : f      =     10.6 : 1.0\n",
      "                  suffix = 'ti'                f : m      =      9.8 : 1.0\n",
      "                  suffix = 'im'                m : f      =      9.7 : 1.0\n",
      "                  suffix = 'as'                m : f      =      9.5 : 1.0\n",
      "                  suffix = 'ud'                m : f      =      9.4 : 1.0\n",
      "                  suffix = 'ya'                f : m      =      9.4 : 1.0\n",
      "                  suffix = 'di'                f : m      =      8.8 : 1.0\n",
      "                  suffix = 'er'                m : f      =      8.4 : 1.0\n",
      "                  suffix = 'ff'                m : f      =      8.3 : 1.0\n",
      "                  suffix = 'st'                m : f      =      8.3 : 1.0\n",
      "                  suffix = 'ip'                m : f      =      8.3 : 1.0\n",
      "                  suffix = 'ul'                m : f      =      8.3 : 1.0\n",
      "                  suffix = 'rk'                m : f      =      8.3 : 1.0\n",
      "                  suffix = 'go'                m : f      =      8.3 : 1.0\n",
      "                  suffix = 'am'                m : f      =      7.7 : 1.0\n",
      "                  suffix = 'em'                m : f      =      7.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the suffix is by far more important than anything else.  Therefore, we try a *brute force* approach and increase the length of the suffix feature to three characters.  After all three is more than two, so this should be an improvement.  However, we have to take care of the fact that some names have a length of just two characters.  Our new implementation of `gender_features` deals with this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(name):\n",
    "    features = {}\n",
    "    features[\"first\" ] = name[0].lower()\n",
    "    if len(name) >= 3:\n",
    "        features[\"suffix\"] = name[-3:].lower()\n",
    "    else:\n",
    "        features[\"suffix\"] = name[-2:].lower()\n",
    "    for letter in string.ascii_lowercase:\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_features = [(gender_features(n), g) for (n, g) in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8502857142857143"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, train_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks promising.  It seems that we are on the right track.  Lets check the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7944915254237288"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_features = [(gender_features(n), g) for (n, g) in test_set]\n",
    "nltk.classify.accuracy(classifier, test_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, our new features <em style=\"color:blue;\">overfit</em> the training data and do not generalize.  Hence, we conclude that having a suffix of three characters is not helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final attempt to solve to improve the accuracy contains three ideas:\n",
    "<ol>\n",
    "    <li>Instead of just using the first character as a feature, we should use the first two characters. \n",
    "        After all, we are also using the last two characters.\n",
    "    </li>\n",
    "    <li>Often, the way the vowels of a name are connected gives a hint about the gender. </li>\n",
    "    <li>In the same way, the consonants might be helpful.  However, we will only use the set of all consonants\n",
    "        occurring in a name, not the order in which they appear.\n",
    "    </li>\n",
    "</ol>\n",
    "Furthermore, in order to reduce the overfitting we will drop the features that check the occurrence of each character individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `find_vowels`$(s)$ takes a string $s$ and strips out all characters that are not vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vowels(s):\n",
    "    return ''.join([c for c in s if c in 'aeiouy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uo'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_vowels('Hugo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `find_consonants`$(s)$ takes a string $s$ and returns a set of its consonants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consonants(s):\n",
    "    return frozenset({c for c in s if c not in 'aeiouy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'H', 'g'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_consonants('Hugo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(name):\n",
    "    name     = name.lower()\n",
    "    features = {}\n",
    "    features[\"first\" ] = name[:2]\n",
    "    features[\"suffix\"] = name[-2:]\n",
    "    features[\"vowels\"] = find_vowels(name)\n",
    "    features[\"consonants\"] = find_consonants(name)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667142857142857"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_features = [(gender_features(n), g) for (n, g) in train_set]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_features)\n",
    "nltk.classify.accuracy(classifier, train_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8273305084745762"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_features = [(gender_features(n), g) for (n, g) in test_set]\n",
    "nltk.classify.accuracy(classifier, test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  suffix = 'na'                f : m      =     93.2 : 1.0\n",
      "                  suffix = 'la'                f : m      =     70.2 : 1.0\n",
      "                  suffix = 'ia'                f : m      =     37.8 : 1.0\n",
      "                  suffix = 'sa'                f : m      =     33.8 : 1.0\n",
      "                  suffix = 'us'                m : f      =     28.3 : 1.0\n",
      "                  suffix = 'io'                m : f      =     26.1 : 1.0\n",
      "                  suffix = 'ta'                f : m      =     24.1 : 1.0\n",
      "                  suffix = 'ra'                f : m      =     23.9 : 1.0\n",
      "                  suffix = 'rt'                m : f      =     23.6 : 1.0\n",
      "                  suffix = 'rd'                m : f      =     23.5 : 1.0\n",
      "                  vowels = 'eo'                m : f      =     22.6 : 1.0\n",
      "                  vowels = 'uo'                m : f      =     17.9 : 1.0\n",
      "                   first = 'hu'                m : f      =     16.2 : 1.0\n",
      "                  suffix = 'ch'                m : f      =     15.0 : 1.0\n",
      "                  vowels = 'eao'               m : f      =     14.7 : 1.0\n",
      "                  suffix = 'ea'                f : m      =     13.8 : 1.0\n",
      "              consonants = frozenset({'l', 'n', 'j'})      f : m      =     13.4 : 1.0\n",
      "              consonants = frozenset({'r', 'g', 'd'})      m : f      =     13.0 : 1.0\n",
      "                  suffix = 'ka'                f : m      =     13.0 : 1.0\n",
      "                   first = 'ya'                m : f      =     12.8 : 1.0\n",
      "                  suffix = 'on'                m : f      =     11.1 : 1.0\n",
      "                  vowels = 'io'                m : f      =     11.0 : 1.0\n",
      "                  suffix = 'ns'                m : f      =     10.6 : 1.0\n",
      "              consonants = frozenset({'s', 'r', 'n', 'b'})      m : f      =      9.9 : 1.0\n",
      "              consonants = frozenset({'l', 'n', 'd', 'w'})      m : f      =      9.9 : 1.0\n",
      "                  suffix = 'ti'                f : m      =      9.8 : 1.0\n",
      "                   first = 'fo'                m : f      =      9.7 : 1.0\n",
      "                  suffix = 'im'                m : f      =      9.7 : 1.0\n",
      "                  suffix = 'as'                m : f      =      9.5 : 1.0\n",
      "                   first = 'tu'                m : f      =      9.5 : 1.0\n",
      "                  suffix = 'ud'                m : f      =      9.4 : 1.0\n",
      "                  suffix = 'ya'                f : m      =      9.4 : 1.0\n",
      "                  vowels = 'eau'               m : f      =      9.2 : 1.0\n",
      "              consonants = frozenset({'r', 'd', 'w'})      m : f      =      8.9 : 1.0\n",
      "              consonants = frozenset({'s', 'l', 'w'})      m : f      =      8.9 : 1.0\n",
      "                  vowels = 'aiee'              f : m      =      8.8 : 1.0\n",
      "                  suffix = 'di'                f : m      =      8.8 : 1.0\n",
      "              consonants = frozenset({'n'})      f : m      =      8.6 : 1.0\n",
      "                  suffix = 'er'                m : f      =      8.4 : 1.0\n",
      "                  suffix = 'ip'                m : f      =      8.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our features occur in the list of the 30 most important features.  In order to improve our model we could try to use a classifier that is different from `NaiveBayesClassifier`.  For example, the `MaxentClassifier` is more sophisticated than the `NaiveBayesClassifier`.  However, this classifier also takes a much longer time to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.39765        0.815\n",
      "             3          -0.33096        0.853\n",
      "             4          -0.29750        0.864\n",
      "             5          -0.27676        0.871\n",
      "             6          -0.26228        0.875\n",
      "             7          -0.25140        0.879\n",
      "             8          -0.24280        0.882\n",
      "             9          -0.23577        0.884\n",
      "            10          -0.22986        0.886\n",
      "            11          -0.22479        0.887\n",
      "            12          -0.22038        0.889\n",
      "            13          -0.21649        0.889\n",
      "            14          -0.21302        0.891\n",
      "            15          -0.20991        0.892\n",
      "            16          -0.20708        0.893\n",
      "            17          -0.20451        0.893\n",
      "            18          -0.20215        0.893\n",
      "            19          -0.19997        0.895\n",
      "            20          -0.19796        0.895\n",
      "            21          -0.19609        0.896\n",
      "            22          -0.19434        0.897\n",
      "            23          -0.19271        0.897\n",
      "            24          -0.19118        0.898\n",
      "            25          -0.18974        0.898\n",
      "            26          -0.18838        0.898\n",
      "            27          -0.18709        0.899\n",
      "            28          -0.18588        0.899\n",
      "            29          -0.18472        0.899\n",
      "            30          -0.18362        0.900\n",
      "            31          -0.18258        0.900\n",
      "            32          -0.18158        0.900\n",
      "            33          -0.18063        0.900\n",
      "            34          -0.17972        0.900\n",
      "            35          -0.17885        0.900\n",
      "            36          -0.17801        0.901\n",
      "            37          -0.17721        0.901\n",
      "            38          -0.17643        0.901\n",
      "            39          -0.17569        0.901\n",
      "            40          -0.17498        0.901\n",
      "            41          -0.17429        0.902\n",
      "            42          -0.17362        0.902\n",
      "            43          -0.17298        0.902\n",
      "            44          -0.17236        0.902\n",
      "            45          -0.17176        0.902\n",
      "            46          -0.17118        0.902\n",
      "            47          -0.17062        0.902\n",
      "            48          -0.17008        0.902\n",
      "            49          -0.16955        0.903\n",
      "            50          -0.16904        0.903\n",
      "            51          -0.16854        0.903\n",
      "            52          -0.16806        0.903\n",
      "            53          -0.16759        0.903\n",
      "            54          -0.16714        0.903\n",
      "            55          -0.16669        0.903\n",
      "            56          -0.16626        0.903\n",
      "            57          -0.16584        0.903\n",
      "            58          -0.16543        0.903\n",
      "            59          -0.16503        0.903\n",
      "            60          -0.16464        0.904\n",
      "            61          -0.16427        0.903\n",
      "            62          -0.16390        0.903\n",
      "            63          -0.16354        0.904\n",
      "            64          -0.16318        0.904\n",
      "            65          -0.16284        0.904\n",
      "            66          -0.16250        0.903\n",
      "            67          -0.16217        0.903\n",
      "            68          -0.16185        0.904\n",
      "            69          -0.16154        0.904\n",
      "            70          -0.16123        0.904\n",
      "            71          -0.16093        0.904\n",
      "            72          -0.16064        0.904\n",
      "            73          -0.16035        0.904\n",
      "            74          -0.16006        0.904\n",
      "            75          -0.15979        0.904\n",
      "            76          -0.15952        0.904\n",
      "            77          -0.15925        0.905\n",
      "            78          -0.15899        0.905\n",
      "            79          -0.15873        0.905\n",
      "            80          -0.15848        0.905\n",
      "            81          -0.15824        0.905\n",
      "            82          -0.15800        0.905\n",
      "            83          -0.15776        0.906\n",
      "            84          -0.15753        0.906\n",
      "            85          -0.15730        0.906\n",
      "            86          -0.15707        0.906\n",
      "            87          -0.15685        0.906\n",
      "            88          -0.15664        0.906\n",
      "            89          -0.15642        0.906\n",
      "            90          -0.15621        0.906\n",
      "            91          -0.15601        0.906\n",
      "            92          -0.15581        0.906\n",
      "            93          -0.15561        0.906\n",
      "            94          -0.15541        0.906\n",
      "            95          -0.15522        0.906\n",
      "            96          -0.15503        0.906\n",
      "            97          -0.15484        0.906\n",
      "            98          -0.15466        0.906\n",
      "            99          -0.15448        0.906\n",
      "         Final          -0.15430        0.906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9061428571428571"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_features = [(gender_features(n), g) for (n, g) in train_set]\n",
    "classifier = nltk.MaxentClassifier.train(train_set_features)\n",
    "nltk.classify.accuracy(classifier, train_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like an improvement.  Let's check the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8326271186440678"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_features = [(gender_features(n), g) for (n, g) in test_set]\n",
    "nltk.classify.accuracy(classifier, test_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the improvement is not real: It is mostly overfitting.  The same thing happens if we try the `ConditionalExponentialClassifier`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**:  Try to design features that improve the accuracy of the classifier on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
