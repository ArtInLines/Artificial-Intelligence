{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100% !important; } </style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we want to investigate is stored in the file `'fake-data.csv'`.   It is data that I have found somewhere.  I am not sure whether this data is real or fake.  Therefore, I won't discuss the attributes of the data.  The point of the data is that it is a classification problem that can not be solved with \n",
    "ordinary logistic regression.  We will introduce <em style=\"color:blue;\">polynomial logistic regression</em> to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF = pd.read_csv('fake-data.csv')\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the features from the data frame and convert it into a `NumPy` <em style=\"color:blue;\">feature matrix</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(DF[['x','y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the target column and convert it into a `NumPy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(DF['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the instances according to their class we divide the feature matrix $X$ into two parts. $\\texttt{X_pass}$ contains those examples that have class $1$, while $\\texttt{X_fail}$ contains those examples that have class $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pass = np.array(DF.loc[Y == 1.0])\n",
    "X_fail = np.array(DF.loc[Y == 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split the data into a <em style=\"color:blue;\">training set</em> and a <em style=\"color:blue;\">test set</em>.\n",
    "The <em style=\"color:blue;\">training set</em> will be used to compute the parameters of our model, while the\n",
    "<em style=\"color:blue;\">testing set</em> is only used to check the *accuracy*.  SciKit-Learn has a predefined method\n",
    "`sklearn.model_selection import train_test_split` that can be used to randomly split data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data at a ratio of $4:1$, i.e. $80\\%$ of the data will be used for training, while the remaining $20\\%$ is used to test the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a <em style=\"color:blue;\">logistic regression</em> classifier, we import the module `linear_model` from SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{logistic_regression}(\\texttt{X_train}, \\texttt{Y_train}, \\texttt{X_test}, \\texttt{Y_test})$ takes a feature matrix $\\texttt{X_train}$ and a corresponding vector $\\texttt{Y_train}$ and computes a logistic regression model $M$ that best fits these data.  Then, the accuracy of the model is computed using the test data $\\texttt{X_test}$ and $\\texttt{Y_test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, Y_train, X_test, Y_test):\n",
    "    model       = lm.LogisticRegression(C=10000, tol=1e-6, solver='newton-cg')\n",
    "    M           = model.fit(X_train, Y_train)\n",
    "    train_score = M.score(X_train, Y_train)\n",
    "    yPredict    = M.predict(X_test)\n",
    "    accuracy    = np.sum(yPredict == Y_test) / len(Y_test)\n",
    "    return M, train_score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to build a model for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X_train, Y_train, X_test, Y_test)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are only two classes, the accuracy of our first model is quite poor.  \n",
    "Let us extract the coefficients so we can plot the <em style=\"color:blue;\">decision boundary</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0     = M.intercept_\n",
    "ϑ1, ϑ2 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') \n",
    "H = np.arange(0.45, 0.9, 0.05)\n",
    "P = -(ϑ0 + ϑ1 * H)/ϑ2\n",
    "plt.plot(H, P, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, pure <em style=\"color:blue;\">logistic regression</em> is not working for this example.  The reason is, that a linear decision boundary is not \n",
    "able to separate the positive examples from the negative examples.  Let us add <em style=\"color:blue;\">polynomial features</em>.  This enables us to create \n",
    "more complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{extend}(X)$ takes a feature matrix $X$ that is supposed to contain two features $x$ and $y$.  It creates the new features $x^2$, $y^2$ and $x\\cdot y$ and returns a new feature matrix that also contains these additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend(X):\n",
    "    n  = len(X)\n",
    "    fx = np.reshape(X[:,0], (n, 1))\n",
    "    fy = np.reshape(X[:,1], (n, 1))\n",
    "    return np.hstack([fx, fy, fx*fx, fy*fy, fx*fy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quadratic = extend(X_train)\n",
    "X_test_quadratic  = extend(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X_train_quadratic, Y_train, X_test_quadratic, Y_test)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work better.  Let us compute the decision boundary and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0                 = M.intercept_\n",
    "ϑ1, ϑ2, ϑ3, ϑ4, ϑ5 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is now given by the following equation:\n",
    "$$ \\vartheta_0 + \\vartheta_1 \\cdot x + \\vartheta_2 \\cdot y + \\vartheta_3 \\cdot x^2 + \\vartheta_4 \\cdot y^2 + \\vartheta_5 \\cdot x \\cdot y = 0$$\n",
    "This is the equation of an ellipse.  Let us plot the <em style=\"color:blue;\">decision boundary</em> with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a    = np.arange(-1.0, 1.0, 0.005)\n",
    "b    = np.arange(-1.0, 1.0, 0.005)\n",
    "A, B = np.meshgrid(a,b)\n",
    "Z    = ϑ0 + ϑ1 * A + ϑ2 * B + ϑ3 * A * A + ϑ4 * B * B + ϑ5 * A * B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') \n",
    "CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to add <em style=\"color:blue;\">quartic features</em> next.  These are features like $x^4$, $x^2\\cdot y^2$, etc.\n",
    "Luckily, SciKit-Learn has function that can automize this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartic = PolynomialFeatures(4, include_bias=False)\n",
    "X_train_quartic = quartic.fit_transform(X_train)\n",
    "X_test_quartic  = quartic.fit_transform(X_test)\n",
    "print(quartic.get_feature_names(['x', 'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit the quartic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X_train_quartic, Y_train, X_test_quartic, Y_test)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0 = M.intercept_\n",
    "ϑ1, ϑ2, ϑ3, ϑ4, ϑ5, ϑ6, ϑ7, ϑ8, ϑ9, ϑ10, ϑ11, ϑ12, ϑ13, ϑ14 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a    = np.arange(-1.0, 1.0, 0.005)\n",
    "b    = np.arange(-1.0, 1.0, 0.005)\n",
    "A, B = np.meshgrid(a,b)\n",
    "Z    = ϑ0 + ϑ1 * A + ϑ2 * B + ϑ3 * A**2 + ϑ4 * A * B + ϑ5 * B**2 + ϑ6 * A**3 + ϑ7 * A**2 * B + ϑ8 * A * B**2 + ϑ9 * B**3 + \\\n",
    "       ϑ10 * A**4 + ϑ11 * A**3 * B + ϑ12 * A**2 * B**2 + ϑ13 * A * B**3 + ϑ14 * B**4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') \n",
    "CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get bold and try feature up to the sixth power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sextic         = PolynomialFeatures(6, include_bias=False)\n",
    "X_train_sextic = sextic.fit_transform(X_train)\n",
    "X_test_sextic  = sextic.fit_transform(X_test)\n",
    "print(sextic.get_feature_names(['x', 'y']))\n",
    "len(sextic.get_feature_names(['x', 'y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X_train_sextic, Y_train, X_test_sextic, Y_test)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0 = M.intercept_\n",
    "ϑ1, ϑ2, ϑ3, ϑ4, ϑ5, ϑ6, ϑ7, ϑ8, ϑ9, ϑ10, ϑ11, ϑ12, ϑ13, ϑ14, ϑ15, ϑ16, ϑ17, ϑ18, ϑ19, ϑ20, ϑ21, ϑ22, ϑ23, ϑ24, ϑ25, ϑ26, ϑ27 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a    = np.arange(-1.0, 1.0, 0.005)\n",
    "b    = np.arange(-1.0, 1.3, 0.005)\n",
    "A, B = np.meshgrid(a,b)\n",
    "Z    = ϑ0 + \\\n",
    "       ϑ1  * A    + ϑ2  * B + \\\n",
    "       ϑ3  * A**2 + ϑ4  * A    * B + ϑ5  * B**2 + \\\n",
    "       ϑ6  * A**3 + ϑ7  * A**2 * B + ϑ8  * A    * B**2 + ϑ9  * B**3 + \\\n",
    "       ϑ10 * A**4 + ϑ11 * A**3 * B + ϑ12 * A**2 * B**2 + ϑ13 * A * B**3    + ϑ14 * B**4 + \\\n",
    "       ϑ15 * A**5 + ϑ16 * A**4 * B + ϑ17 * A**3 * B**2 + ϑ18 * A**2 * B**3 + ϑ19 * A * B**4    + ϑ20 * B**5 + \\\n",
    "       ϑ21 * A**6 + ϑ22 * A**5 * B + ϑ23 * A**4 * B**2 + ϑ24 * A**3 * B**3 + ϑ25 * A**2 * B**4 + ϑ26 * A * B**5 + ϑ27 * B**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pass_train = X_train[Y_train == 1.0]\n",
    "X_fail_train = X_train[Y_train == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.11, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.21, step=0.1))\n",
    "plt.scatter(X_pass_train[:,0], X_pass_train[:,1], color='b') \n",
    "plt.scatter(X_fail_train[:,0], X_fail_train[:,1], color='r') \n",
    "CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "octic         = PolynomialFeatures(8, include_bias=False)\n",
    "X_train_octic = octic.fit_transform(X_train)\n",
    "X_test_octic  = octic.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X_train_octic, Y_train, X_test_octic, Y_test)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Θ = [M.intercept_] + list(M.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a    = np.arange(-1.0, 1.0, 0.005)\n",
    "b    = np.arange(-1.0, 1.3, 0.005)\n",
    "A, B = np.meshgrid(a,b)\n",
    "Z    = Θ[0] + \\\n",
    "       Θ[1]  * A    + Θ[2]  * B + \\\n",
    "       Θ[3]  * A**2 + Θ[4]  * A    * B + Θ[5]  *        B**2 + \\\n",
    "       Θ[6]  * A**3 + Θ[7]  * A**2 * B + Θ[8]  * A    * B**2 + Θ[9]  *        B**3 + \\\n",
    "       Θ[10] * A**4 + Θ[11] * A**3 * B + Θ[12] * A**2 * B**2 + Θ[13] * A    * B**3 + Θ[14] *        B**4 + \\\n",
    "       Θ[15] * A**5 + Θ[16] * A**4 * B + Θ[17] * A**3 * B**2 + Θ[18] * A**2 * B**3 + Θ[19] * A    * B**4 + Θ[20] *        B**5 + \\\n",
    "       Θ[21] * A**6 + Θ[22] * A**5 * B + Θ[23] * A**4 * B**2 + Θ[24] * A**3 * B**3 + Θ[25] * A**2 * B**4 + Θ[26] * A    * B**5 + Θ[27] *        B**6 + \\\n",
    "       Θ[28] * A**7 + Θ[29] * A**6 * B + Θ[30] * A**5 * B**2 + Θ[31] * A**4 * B**3 + Θ[32] * A**3 * B**4 + Θ[33] * A**2 * B**5 + Θ[34] * A    * B**6 + Θ[35] *     B**7 + \\\n",
    "       Θ[36] * A**8 + Θ[37] * A**7 * B + Θ[38] * A**6 * B**2 + Θ[39] * A**5 * B**3 + Θ[40] * A**4 * B**4 + Θ[41] * A**3 * B**5 + Θ[42] * A**2 * B**6 + Θ[43] * A * B**7 + \\\n",
    "       Θ[44] * B**8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\texttt{polynomial}(n)$ creates a polynomial in the variables `A` and `B` that contains all terms of the form $\\Theta[k] \\cdot A^i \\cdot B^j$ where $i+j \\leq n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = polynomial_grid(8, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.11, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.21, step=0.1))\n",
    "plt.scatter(X_pass_train[:,0], X_pass_train[:,1], color='b') \n",
    "plt.scatter(X_fail_train[:,0], X_fail_train[:,1], color='r') \n",
    "CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(n):\n",
    "    sum = 'Θ[0]' \n",
    "    cnt = 0\n",
    "    for k in range(1, n+1):\n",
    "        for i in range(0, k+1):\n",
    "            cnt += 1\n",
    "            sum += f' + Θ[{cnt}] * A**{k-i} * B**{i}'\n",
    "    print('number of features:', cnt)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_grid(n, M):\n",
    "    Θ    = [M.intercept_] + list(M.coef_[0])\n",
    "    a    = np.arange(-1.0, 1.0, 0.005)\n",
    "    b    = np.arange(-1.0, 1.3, 0.005)\n",
    "    A, B = np.meshgrid(a,b)\n",
    "    return eval(polynomial(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nth_degree_boundary(n):\n",
    "    poly         = PolynomialFeatures(n, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly  = poly.fit_transform(X_test)\n",
    "    M, score, accuracy = logistic_regression(X_train_poly, Y_train, X_test_poly, Y_test)\n",
    "    print('The accuracy on the training set is:', score)\n",
    "    print('The accuracy on the test     set is:', accuracy)\n",
    "    Z = polynomial_grid(n, M)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.title('A Classification Problem')\n",
    "    plt.axvline(x=0.0, c='k')\n",
    "    plt.axhline(y=0.0, c='k')\n",
    "    plt.xlabel('x axis')\n",
    "    plt.ylabel('y axis')\n",
    "    plt.xticks(np.arange(-0.9, 1.11, step=0.1))\n",
    "    plt.yticks(np.arange(-0.8, 1.21, step=0.1))\n",
    "    plt.scatter(X_pass_train[:,0], X_pass_train[:,1], color='b') \n",
    "    plt.scatter(X_fail_train[:,0], X_fail_train[:,1], color='r') \n",
    "    CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_nth_degree_boundary(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
