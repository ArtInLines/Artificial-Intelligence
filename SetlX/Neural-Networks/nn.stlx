rndVector := length   |-> la_vector([random() - 0.5: i in [1..length]]);
my_rnd    := []       |-> (random() - 0.5) / 14; // 14 = 28 / 2
rndMatrix := [rs, cs] |-> la_matrix([[my_rnd(): c in [1..cs]]: r in [1..rs]]);
sigmoid   := [x]      |-> la_vector([1 / (1 + exp(-a)) : a in x]);
// derivative of the sigmoid of a vector
sigmoid_prime := procedure(x) {
    s := sigmoid(x); 
    return la_vector([a * (1 - a): a in s]);
};
hadamard := [x, y] |-> la_vector([x[i] * y[i]: i in [1 .. #x]]);
// compute the index of the biggest value in x
argmax := procedure(x) {
    [maxValue, maxIndex] := [x[1], 1];
    for (i in [2 .. #x] | x[i] > maxValue) {
        [maxValue, maxIndex] := [x[i], i];
    }
    return maxIndex;
};

class network(inputSize, hiddenSize, outputSize) {
    mInputSize  := inputSize;   //        784
    mHiddenSize := hiddenSize;  //  30 .. 100
    mOutputSize := outputSize;  //         10
    mBiasesH    := rndVector(mHiddenSize);  // biases hidden layer
    mBiasesO    := rndVector(mOutputSize);  // biases output layer
    mWeightsH   := rndMatrix(mHiddenSize, mInputSize);  // weights hidden layer
    mWeightsO   := rndMatrix(mOutputSize, mHiddenSize); // weights output layer
    
    // epochs: number of epochs
    // mbs:    minibatch size
    // eta:    learning rate
    sgd := procedure(training_data, epochs, mbs, eta, test_data) {
        n_test := #test_data;         
        n      := #training_data;
        for (j in [1 .. epochs]) {
            training_data := shuffle(training_data);
            mini_batches  := [training_data[k .. k+mbs-1]: k in [1, mbs..n]];
            for (mini_batch in mini_batches) {
                update_mini_batch(mini_batch, eta);
            } 
            print("Epoch $j$: $evaluate(test_data)$ / $n_test$");
        }
    };
    update_mini_batch := procedure(mini_batch, eta) {
        nabla_BH := 0 * mBiasesH;  // gradient biases hidden layer
        nabla_BO := 0 * mBiasesO;  // gradient biases output layer
        nabla_WH := 0 * mWeightsH; // gradient weights hidden layer
        nabla_WO := 0 * mWeightsO; // gradient weights output layer
        for([x,y] in mini_batch) {
            [dltNbl_BH, dltNbl_BO, dltNbl_WH, dltNbl_WO] := backprop(x, y);
            nabla_BH += dltNbl_BH;
            nabla_BO += dltNbl_BO;
            nabla_WH += dltNbl_WH;
            nabla_WO += dltNbl_WO;
        }        
        alpha := eta / #mini_batch;
        this.mBiasesH  -= alpha * nabla_BH;
        this.mBiasesO  -= alpha * nabla_BO;
        this.mWeightsH -= alpha * nabla_WH;
        this.mWeightsO -= alpha * nabla_WO;
    };
    // Backpropagation to calculate the gradient for the cost function
    // x: training inputs
    // y: correct result for inputs x
    backprop := procedure(x, y) {
        // feedforward pass
        ZH := mWeightsH * x + mBiasesH;
        AH := sigmoid(ZH);
        ZO := mWeightsO * AH + mBiasesO;
        AO := sigmoid(ZO);
        // backwards pass, output layer
        epsilonO := hadamard(AO - y, sigmoid_prime(ZO));
        nabla_BO := epsilonO;
        nabla_WO := la_matrix(epsilonO) * la_matrix(AH)!;
        // backwards pass, hidden layer
        epsilonH := hadamard(mWeightsO! * epsilonO, sigmoid_prime(ZH));
        nabla_BH := epsilonH;
        nabla_WH := la_matrix(epsilonH) * la_matrix(x)!;
        return [nabla_BH, nabla_BO, nabla_WH, nabla_WO];
    };
    // Returns sum of correct guesses after feedforwarding
    evaluate := procedure(test_data) {
        test_results := [[argmax(feedforward(x)) - 1, y]: [x, y] in test_data];
        // Return sum of correct guesses
        return #[1 : [a, b] in test_results | a == b];
    };
    feedforward := procedure(x) {
        AH := sigmoid(mWeightsH * x  + mBiasesH);
        AO := sigmoid(mWeightsO * AH + mBiasesO);
        return AO;
    };
}
